S3 101
  - S3 provides developers and IT teams with secure, durable, highly-scalable object storage
  - S3 has a simple web service interface to store and retrieve any amount of data from anywhere on the web
  - S3 is not for operating systems or Databases
  - S3 is a safe place to store your files
  - S3 is object storage based, not block storage based
  - With S3, your data is spread across multiple devices and facilities
  - S3 is object based - i.e. allows you to upload files
  - S3 files can be from 0 Bytes to 5 TB
  - S3 has unlimited storage
  - With S3, files are stored in buckets, which are similar to folders
  - S3 has a universal name space, that is, bucket names must be unique globally; similar to a DNS address
  - When you upload a file into S3 you will receive an HTTP 200 code if the upload was successful
  - S3 Data Consistency: two modes
    - Read after Write consistency for PUTS of new objects
    - Eventual consistency for overwrite PUTS and DELETES (can take some time to propagate)
  - S3 is a simple key-value store
    - S3 is object based; objects consist of the following:
      - a Key, which is simply the name of the object
      - a Value, which is simply the data, which is made up of a series of bytes
      - a Version ID, which is important for versioning
      - Metadata, which is data about the data you are storing
      - Subresources, which are bucket-specific configuration:
        - Bucket policies
        - Access Control Lists
        - Cross Origin Resource Sharing (CORS); files located in one bucket to access files located in another bucket
        - Transfer Acceleration, which is a service which allows you to accelerate file transfers when upload many files into S3
  - S3 is built for 99.99% availability
  - Amazon guarantees 99.9% availability
  - Amazon guarantees 99.999999999% durability for S3 information (11 9s)
  - S3 has tiered storage available
  - S3 has lifecycle management
  - S3 has versioning
  - S3 has encryption
  - You can secure access to your data using:
    - Access Control Lists
    - Bucket Policies
  - S3 Storage Tiers
    - Regular S3
      - 99.99% availability, 99.999999999% durability, stored redundantly across multiple devices in multiple facilities;
        designed to sustain a loss of 2 facilities concurrently
    - S3 IA (Infrequently Accessed)
      - For data that is accessed less frequently, but requires rapid access when needed
      - Lower fee than S3, but you are charged a retrieval fee every time you retrieve the data
    - One-Zone IA
      - Same as IA however data is stored in a single AZ only
      - Still 99.999999999% durability
      - Only 99.5% availability
      - Costs 20% less than regular IA
    - Reduced Redundancy Storage
      - Designed to provide 99.99% durability and 99.99% availability of objects over a given year
      - Used for data that can be recreated if lost, e.g. thumbnails
    - Glacier
      - Very inexpensive, but used for archival only
      - Optimized for data that is infrequently accessed
      - It takes 3 to 5 hours to restore data from Glacier
    - S3 Intelligent Tiering
      - Suitable for data which has unknown or unpredictable access patterns
      - 2 Tiers
        - frequent access
        - infrequent access
      - Automatically moves your data to the most cost-effective tier based on how frequently you access each object within a bucket
      - If an object is not accessed for 30 consecutive days, it gets automatically moved to the infrequent access tier
      - As soon as an object in the infrequent access tier is accessed, it is moved to the frequent access tier
      - 99.99999999999% durability
      - 99.9% availability over a given year
      - Helps optimize cost
      - No fees for accessing your data but a small monthly fee for monitoring/automation $0.0025 per 1,000 objects
 - S3 Charges
   - Storage per GB
   - Requests (Get, Put, Copy, etc.)
   - Storage Management pricing
     - Inventory, Analytics, and Object Tags
   - Data management pricing
     - Data transferred out of S3
     - Free to transfer in to S3
   - Transfer Acceleration
     - Use of CloudFront to optimize transfers
 - Exam Tips
   - Example S3 bucket url: https://s3-eu-west-1.amazonaws.com/bucketname
S3 Security
 - By default, all newly created buckets are private
 - You can setup access control to your buckets using:
   - Bucket Policies: applied at the bucket level
   - Access Control Lists: applied at the object level
 - S3 buckets can be configured to create access logs, which log all requests made to the S3 bucket. These logs can be written to another bucket
 - Bucket policies are written in JSON
 - All versions of a versioned S3 bucket are kept in the same bucket
 - S3 object-level logging records any api level activity using AWS CloudTrail, at an additional cost
 - S3 has two different types of encryption:
   - AES-256: uses server-side encryption with AWS S3-Managed Keys (SSE-S3)
   - AWS-KMS: uses server-side encryption with AWS KMS-Managed Keys (SSE-KMS)
 - You can enable S3 CloudWatch request metrics, which monitor requests in your bucket for an additional cost
   - gives performance metrics for S3
 - S3 has public access settings
   - Used to enforce that buckets do not allow public access to data.
   - You can also configure S3 to block public access at the account level.
 - You can grant public read-access to objects in the Access Control List
 - You can encrypt at rest at the bucket level using bucket policies or the object level using access control lists
 - If the bucket policy does not allow public access, you cannot upload an object into the bucket and make that object publicly accessible using the Access Control List
S3 Encryption
 - In transit
   - SSL/TLS (Transport Layer Security, will replace SSL)
     - generally means using https to transmit data
 - At Rest
   - Server-side encryption - 3 types
     - S3 managed keys - SSE-S3
       - AWS manages the keys for you, rotate keys on a defined frequency
       - uses AES-256 bit encryption
     - AWS Key Management Service, managed keys - SSE-KMS
       - AWS manages the keys for you
       - uses an envelope key that encrypts your encryption key
       - gives an audit trail which records the use of your encryption key
       - can use your own key, or the default AWS key
     - Server-side encryption with customer provided keys - SSE-C
       - AWS manages the encryption and decryption but you manage your own keys
 - Client-side encryption
   - you encrypt the files yourself before you upload to S3
 - Enforcement of S3 Encryption
   - Every time a file is uploaded to S3, a PUT request is initiated
   - If the file is to be encrypted at upload time, the x-amz-server-side-encryption parameter will be included in the request header
     - two options are currently available
       - x-amz-server-side-encryption: AES256 (SSE-S3 - S3 managed keys)
       - x-amz-server-side-encryption: ams:kms (SSE-KMS - KMS managed keys)
     - when this parameter is included in the header of the PUT request, it tells S3 to encrypt the object at the time of upload, using the specified encryption method
     - you can enforce the use of server-side encryption by using a bucket policy which denies any S3 PUT request which does not include the x-amz-server-side-encryption
       parameter in the request header
CloudFront
 - A Content Delivery Network (CDN) is a system of distributed servers (network) that deliver webpages and other web content to a user based on the:
   - geographic location of the user
   - the origin of the web page
   - a content delivery server
 - Edge Locations are collections of servers in dispersed geographic locations
   - used by CloudFront to cache web content close to geographically dispersed users
   - can write content into Edge Locations (PUT an object to them) as well as read from them
   - separate from AZs or Regions
 - Origin
   - the origin of all the files that the CDN will distribute
   - Origins can be S3 buckets, EC2 instances, an ELB, or Route53
 - Distribution
   - the name given to the CDN
   - consists of a collection of edge locations
   - Two types of distributions
     - Web Distribution
       - Typically used for websites
     - RTMP (Real Time Messaging Protocol)
       - Used for media streaming
       - Adobe Flash, multi-media content
 - CloudFront also works with any non-AWS origin server
 - CloudFront accelerates S3 Transfer Acceleration
   - S3 Transfer Acceleration takes advantage of CloudFront's globally distributed edge locations
   - When the data being transferred arrives at an edge location the remaining transfer moves over AWS's optimized network path
 - You can clear cached objects from your CloudFront edge locations but you will be charged
 - Can have multiple origins for a distribution
 - You can clear your CloudFront cache by invalidating objects
S3 Performance Optimization (START HERE)
 - S3 is designed to support very high request rates
   - If your S3 buckets are routinely receiving > 100 PUT/LIST/DELETE or > 300 GET requests per second, then there are some best practice
     guidelines that will help optimize S3 Performance
   - The guidance is based on the type of workload you are running
     - GET-intensive workloads
       - use CloudFront content delivery service to get best performance
       - CloudFront will cache your most frequently accessed objects and will reduce latency for your GET requests
     - Mixed Request Type Workloads
       - a mix of GET, PUT, DELETE, GET Bucket - the key names you use for your objects can impact performance workloads
       - S3 uses the key name to determine which partition an object will be stored in
       - The use of sequential key names e.g. names prefixed with a timestamp or alphabetical sequence increases the likelihood
         of having multiple objects stored on the same partition
       - For heavy workloads this can cause I/O issues and contention
       - By using a random prefix to key names (like a hex hash), you can force S3 to distribute your keys across multiple partitions, distributing the I/O workload
Performance Update
 - S3 can support at least 3,500 put requests per second
 - S3 can support at least 5,500 get requests per second
 - Now you no longer need to randomized key names to achieve faster performance
 - Logical and sequential naming patterns can now be used without any performance implication
S3 Summary
 - S3 website url format http://bucketname.s3-website-regionname.amazonaws.com
S3 Quiz
 - S3 provides unlimited storage
   - true (correct)
   - false
 - What is the HTTP code you would see once you successfully place a file in an S3 bucket?
   - 524
   - 200 (correct)
   - 312
   - 404
 - You are hosting a static website in an S3 bucket that uses Java script to reference assets in another S3 bucket. For some reason, these assets are not displaying when users browse to the site. What could be the problem?
   - You haven not enabled Cross Origin Resource Sharing (CORS) on the bucket where the assets are stored (correct)
   - Amazon S3 does not support Javascript
   - You cannot use one S3 bucket to reference another S3 bucket
   - You need to open port 80 on the appropriate security group in which the S3 bucket is located
 - The minimum file size allowed on S3 is 1 byte
   - true
   - false (correct)
 - How does S3 determine which partition to use to store files?
   - The bucket name determines which partition the file is stored in
   - S3 automatically stores your files on a random partition
   - The key name determines which partition the file is stored in (correct)
   - By default, all files in the same bucket are stored on the same partition
 - Your application is consistently reading and writing 100s of objects per second to S3 and your workload is steadily rising. What can you do to achieve the best performance from S3?
   - Configure an additional bucket and distribute the files evenly between the two buckets
   - Configure a CloudFront CDN and use the S3 bucket as the origin
   - Add a hex hash prefix to the objects key name (correct)
   - Add a hex hash suffix to the objects key name
 - What is the maximum file size that can be stored on S3?
   - 4TB
   - 2TB
   - 1TB
   - 5TB (correct)
 - You are using S3 in AP-Northeast to host a static website in a bucket called acloudguru. What would the new URL endpoint be?
   - http://acloudguru.s3-website-ap-southeast-1.amazonaws.com
   - https://s3-ap-northeast-1.amazonaws.com/acloudguru/
   - http://acloudguru.s3-website-ap-northeast-1.amazonaws.com (correct)
   - http://www.acloudguru.s3-website-ap-northeast-1.amazonaws.com
 - When you first create an S3 bucket, this bucket is publicly accessible by default
   - true
   - false (correct)
 - Which feature of AWS can you use to configure S3 to allow one S3 bucket to access files in another S3 bucket?
   - IAM Role
   - CORS (correct)
   - Bucket ACL
   - Bucket Policy
 - If you want to enable a user to download your private data directly from S3, you can insert a pre-signed URL into a web page before giving it to your user.
   - true (correct)
   - false
 - Which of the following encryption methods are supported in S3? (Choose 3)
   - SSE-C (correct)
   - SSE-AES
   - SSE-KMS (correct)
   - SSE-S3 (correct)
 - What is the largest size file you can transfer to S3 using a PUT operation?
   - 1GB
   - 5GB (correct)
   - 100MB
   - 5TB
 - If you encrypt a bucket on S3, what type of encryption does AWS use?
   - International Data Encryption Algorithm (IDEA)
   - Data Encryption Standard (DES)
   - Advanced Encryption Standard (AES) 256 (correct)
   - Advanced Encryption Standard (AES) 128
 - Which of the following options allows users to have secure access to private files located in S3? (Choose 3)
   - CloudFront Origin Access Identity (correct)
   - Public S3 buckets
   - CloudFront Signed URLs (correct)
   - CloudFront Signed Cookies (correct)
Serverless 101

Lambda
  - Lambda is a compute service where you can upload your code and create a lambda function
  - AWS lambda takes care of provisioning and managing the servers that you use to run the code
  - You don't have to worry about operating systems, patching, scaling, etc.
  - You can use lambda in the following ways:
     - As an event-driven compute service where lambda runs your code in response to events
       - These events could be changes to data in an S3 bucket or an DynamoDB table
     - As a compute service to run your code in response to HTTP requests using API Gateway or API calls using AWS SDKs.
  - Lambda code can be written in these languages:
    - Node.js
    - Java
    - Python
    - C#
    - Go
  - Lambda is priced as follows:
    - Number of requests
      - first 1 million requests are free per month
      - $0.20 per 1 million requests after the first 1 million requests
    - Duration
      - Duration is calculated from the time your code begins executing until it returns or otherwise terminates, rounded up to the nearest 100ms
      - The price depends on the amount of memory you allocate to your function
      - You are charged $0.00001667 for every GB-second used
  - Lambda scales out (not up) automatically
  - AWS X-ray allows you to debug lambda function chains
  - Lambda can be used globally
    - you can use a lambda function to back up S3 buckets across regions
API Gateway
 - API Gateway is a fully managed service that lets developers publish, maintain, monitor, and secure APIs at any scale
 - Can use API Gateway to create APIs that access application code running on EC2 instances, code running in Lambda, or any other web application
 - API Gateway exposes HTTPS endpoints to define a RESTful API
 - API Gateway serverless-ly connects to services like Lambda & DynamoDB
 - API Gateway can send each API endpoint to a different target
 - Runs efficiently with low cost
 - Scales effortlessly
 - Track and control usage by API key
 - Throttle requests to prevent attacks
 - Connect to CloudWatch to log all requests for monitoring
 - Maintain multiple versions of your API
 - Defining an API in API Gateway:
   - Define an API (container)
   - Define resources and nested resources (URL paths)
     - For each resource:
       - Select supported HTTP methods (verbs)
       - Set security
       - Choose target (such as EC2, Lambda, DynamoDB, etc.)
       - Set request and response transformations
   - Deploy API to a stage
     - Uses API Gateway domain, by default
     - Can use a custom domain
     - Now supports AWS Certificate Manager: free SSL/TLS certs
 - Can enable API caching
   - Cache your endpoint's response
 - Same-origin-policy
   - allows scripts in a first web page to access data from a second web page only if both pages are served from the same origin
   - used to prevent cross-site scripting
   - enforced by web browsers
 - Cross-Origin-Resource-Sharing (CORS)
   - relaxes the same-origin-policy
   - Allows resources in a web page to be requested from a domain which is different from the domain which served the web page
   - the browser makes an HTTP OPTIONS call for a URL
   - The server returns a response that lists the domains approved for access from the URL
   - Or the server returns an error stating that the origin policy cannot be read at the remote resource, enable CORS on API Gateway
 - Lambda trigger services:
   - API Gateway
   - AWS IoT
   - Alexa Skills Kit
   - Alexa Smart Home
   - CloudFront
   - CloudWatch Events
   - CloudWatch Logs
   - CodeCommit
   - Cognito Sync Trigger
   - DynamoDB
   - Kinesis
   - S3
   - SNS
 - API Gateway integration point types
   - HTTP
   - Lambda Function
   - Mock: return a response purely using API Gateway mappings and transformations
   - AWS Service
   - VPC Link
Version Control with Lambda
   - When you use versioning with Lambda, you can publish one or more versions of your Lambda function
   - Each version of your Lambda function has a unique ARN
   - After you publish a version it is immutable
   - There are two types of ARNs associated with a Lambda function
     - Qualified ARN - The function ARN with the version suffix
       - arn:aws:lambda:aws-region:acct-id:function:hwlloworld:$LATEST
     - Unqualified ARN - The function ARN without the suffix
       - arn:aws:lambda:aws-region:acct-id:function:hwlloworld
   - Alias
     - After publishing your Lambda function, then create an alias to it, such as PROD, you can then use the PROD alias to invoke this version of the function
     - If you make changes and publish this new version, you can point the PROD alias to the new version.
     - If you decide to roll back to the old version, you just repoint the alias to the old version
   - You can use versioning to create splits between traffic, such as blue/green deployments
     - You cannot use $LATEST in version splitting
Step Functions
   - Step Functions allow you to visualize and test your serverless applications
   - Provides a graphical console to arrange and visualize the components of your application as a series of steps
   - Automatically triggers and tracks each step, and retries when there are errors
   - Logs the state of each step, so you can diagnose and debug
   - Can have sequential steps
   - Can have branching steps
   - Can have parallel steps
   - Use Amazon States Language to create steps
X-Ray
   - X-Ray is a service that collects data about requests that your application serves, and provides tools you can use to view, filter, and gain insights into that data
     to identify issues and opportunities for optimization
   - For any traced request to your application, you can see details of the request and response, as well as calls your application makes to downstream
     AWS resources, microservices, databases, and HTTP web APIs
   - Use the X-Ray SDK inside your application
      - the X-Ray SDK sends information (in JSON) to the X-Ray Daemon, which sends the JSON to the X-Ray API
      - the X-Ray creates a visualization of the functioning of your app, which it displays in the X-Ray Console
      - the AWS SDK and the AWS CLI can communicate directly with the X-Ray Daemon and/or the X-Ray API
   - The X-Ray SDK provides
      - Interceptors to add to your code to trace incoming HTTP requests
      - Client handlers to instrument AWS SDK clients that your application uses to call other AWS services
      - An HTTP client to use to instrument calls to other internal and external HTTP web services
   - X-Ray integrates with the following AWS services
      - Elastic Load Balancing
      - Lambda
      - API Gateway
      - EC2
      - Elastic Beanstalk
   - X-Ray supported languages:
      - Java
      - Go
      - Node.js
      - Python
      - Ruby
      - .Net
Advanced API Gateway
  - You can use the API Gateway Import API feature to import an API from an external definition file into API Gateway
  - Currently the API feature supports Swagger v2.0 definition files
  - With the Import API, you can either create a new API by submitting a POST request that includes a Swagger definition in the payload and endpoint configuration,
    or you can update an existing API by using a PUT request that contains a Swagger definition in the payload
  - You can update an API by overwriting it with a new definition, or merge a definition with an existing API
  - You specify the options using a mode query parameter in the request URL
  - API Throttling
    - by default API Gateway limits the steady-state request rate to 10,000 requests per second (rps)
    - The maximum concurrent requests is 5,000 requests across all APIs within an AWS account
    - If you go over 10,000 requests per second or over 5,000 concurrent requests you will receive a 429 Too Many Requests error response
  - SOAP Webservice Passthrough
    - You can configure API Gateway as a SOAP web service passthrough
Serverless Quiz
  - Which of the following services does X-ray integrate with? (Choose 3)
    - Lambda (correct)
    - S3
    - Elastic Load Balancer (correct)
    - API Gateway (correct)
 - You have launched a new web application on AWS using API Gateway, Lambda and S3. Someone posts a thread to reddit about your application and it starts to go viral. You start receiving 10,000 requests every second and you notice that most requests are similar. Your web application begins to struggle. What can you do to optimize performance of your application?
   - Enable API Gateway caching to cache frequent requests (correct)
   - Enable API Gateway Accelerator
   - Migrate your API Gateway to a Network Load Balancer and enable session stickiness for all sessions
   - Change your Route53 alias record to point to AWS Neptune and then configure Neptune to filter your API requests to genuine requests only
 - You have created an application using serverless architecture using Lambda, Api Gateway, S3 and DynamoDB. Your boss asks you to do a major upgrade to API Gateway and you do this and deploy it to production. Unfortunately something has gone wrong and now your application is offline. What should you do to bring your application up as quickly as possible?
   - Restore your previous API gateway configuration using an EBS snapshot
   - Rollback your API Gateway to the previous stage (correct)
   - Restart API Gateway for the new changes to take effect
   - Delete the existing API Gateway
 - You have created a serverless application which converts text into speech using a combination of S3, API Gateway, Lambda, Polly, DynamoDB and SNS. Your users complain that only some text is being converted, whereas longer amounts of text does not get converted. What could be the cause of this problem?
   - Polly has built in censorship, so if you try and send it text that is deemed offensive, it will not generate an MP3
   - You have placed your DynamoDB table in a single availability zone which is currently down, causing an outage
   - AWS X-ray service is interfering with the application and should be disabled
   - Your lambda function needs a longer execution time. You should check how long is needed in the fringe cases and increase the timeout inside the function to slightly longer than that (correct)
 - How does API Gateway deal with legacy SOAP applications
   - Converts the response from the application to XML
   - Provides webservice passthrough for SOAP applications (correct)
   - Converts the response from the application to REST
   - Converts the response from the application to HTML
 - You work for a gaming company that has built a serverless application on AWS using Lambda, API Gateway and DynamoDB. They release a new version of the lambda function and the application stops working. You need to get the application up and back online as fast as possible. What should you do?
   - DynamoDB is not serverless and is causing the error. Migrate your database to RDS and redeploy the lambda function
   - Create a CloudFormation template of the environment. Deploy this template to a separate region and then redirect Route53 to the new region
   - The new function has some dependencies not available to lambda. Redeploy the application on EC2 and put the EC2 instances behind a network load balancer
   - Roll your lambda function back to the previous version (correct)
 - You are developing a new application using serverless infrastructure and are using services such as S3, DynamoDB, Lambda, API Gateway, CloudFront, CloudFormation and Polly. You deploy your application to production and your end users begin complaining about receiving a HTTP 429 error. What could be the cause of the error?
   - Your cloudformation stack is not valid and is failing to deploy properly which is causing a HTTP 429 error
   - You enabled API throttling for a rate limit of 1000 requests per second while in development and now that you have deployed to production your API Gateway is being throttled (correct)
   - You have an S3 bucket policy which is preventing lambda from being able to write files to your bucket, generating a HTTP 429 error
   - Your lambda function does not have sufficient permissions to read to DynamoDB and this is generating a HTTP 429 error
 - You are a developer for a busy real estate company and you want to enable other real estate agents to the ability to show properties on your books, but skinned so that it looks like their own website. You decide the most efficient way to do this is to create is to expose your API to the public. The project works well, however one of your competitors starts abusing this, sending your API tens of thousands of requests per second. This generates a HTTP 429 error. Each agent connects to your API using individual API keys. What actions can you take to stop this behavior?
   - Use AWS Shield Advanced API protection to block the requests
   - Place an AWS Web Application Firewall in front of API gateway and filter the requests
   - Deploy multiple API Gateways and give the agent access to another API Gateway
   - Throttle the agents API access using the individual API Keys (correct)
 - You have designed a serverless learning management system and you are ready to deploy your code in to a test-dev environment. The system uses a combination of Lambda, API Gateway, S3 and CloudFront and is architected to be highly available, fault tolerant and scalable. What are the three different ways you can deploy your code to lambda. (Choose 3)
   - Zip your code into a zip file and upload it via the Lambda console (correct)
   - Import your code to Amazon’s Glacier and change your Lambda execution handler to point to glacier
   - Copy and paste your code in to the integrated development environment (IDE) inside Lambda (correct)
   - Upload your code to S3 and use a MySQL connection string to connect Lambda to your S3 bucket
   - Write a CloudFormation template that will deploy your environment including your code (correct)
 - You have created a simple serverless website using S3, Lambda, API Gateway and DynamoDB. Your website will process the contact details of your customers, predict an expected delivery date of their order and store their order in DynamoDB. You test the website before deploying it into production and you notice that although the page executes, and the lambda function is triggered, it is unable to write to DynamoDB. What could be the cause of this issue?
   - The availability zone that DynamoDB is hosted in is down
   - You have written your function in python which is not supported as a run time environment for Lambda
   - Your lambda function does not have the sufficient Identity Access Management (IAM) permissions to write to DynamoDB (correct)
   - The availability zone that Lambda is hosted in is down
 - You have an internal API that you use for your corporate network. Your company has decided to go all in on AWS to reduce their data center footprint. They will need to leverage their existing API within AWS. What is the most efficient way to do this?
   - Use AWS API Import-Export feature of AWS Storage Gateway
   - Recreate the API manually
   - Use the Swagger Importer tool to import your API in to API Gateway (correct)
   - Replicate your API to API Gateway using the API Replication Master
Introduction to DynamoDB
 - DynamoDB supports both document and key-value data models
 - It is a great fit for mobile, web, gaming, ad-tech, IoT, and many others
 - DynamoDB is serverless
 - Fully Managed
 - Can be configured to auto-scale
 - DynamoDB tables are stored on SSD storage
 - Underlying hardware supporting your DynamoDB tables is spread across 3 geographically distinct datacenters
 - DynamoDB has a choice of two consistency models:
   - Eventually consistent reads (default)
     - Consistency across all the copies of data is usually reached within a second
     - Repeating a read after a short time should return the updated data (best performance)
   - Strongly consistent reads
     - A strongly consistent read returns a result that reflects all writes that received a successful response prior to the read
 - DynamoDB is made of of:
   - Tables
   - Items (think of a row of data)
   - Attributes (think of a column of data)
   - Supports key-value and document data structures
   - Key = the name of the data, Value = the data itself
   - Documents can be written in JSON, XML, or HTML
 - DynamoDB stores and retrieves data based on a Primary Key
 - 2 types of primary key:
   - Partition Key: unique attribute (i.e. userID)
     - The value of the Partition Key is input into an internal hash function which determines the partition or physical location on which the data is stored
     - If you use the Partition Key as your Primary Key, then no two items can have the same Partition Key
   - Composite Key: Partition Key + Sort Key
     - The primary key is a composite key which is a consisting of:
       - a partition key (i.e. userID)
       - a sort key (i.e. the timestamp of the post)
       - 2 items may have the same partition key but the must have different different sort keys
       - All items with the same partition key are stored together then sorted according to the sort key
       - Allows you to store multiple items with the same partition key
 - Access Control to DynamoDB
   - Authentication and access control is managed by IAM policies
   - You can create an IAM user which has specific permissions to access and create DynamoDB tables
   - You can create an IAM role which allows you to obtain temporary access keys which can be used to access DynamoDB
   - You can use a special IAM Condition to an IAM policy to restrict user access to only their own records based on their partition key (i.e. userID)
     - the dynamodb:LeadingKeys parameter in the policy is used to allow users to access only the items where where the partition key value matches their userID
DynamoDB Indexes - Deep Dive
   - DynamoDB supports 2 types of indexes to speed up queries:
     - Local Secondary Index
       - can only be created when you are creating your table
       - you cannot add, remove, or modify it once it has been created
       - it has the same partition key as the table it indexes
       - it has a different sort key than the table it indexes
       - gives you a different view of your data, organized according to an alternative sort key
       - any queries based on this index are much faster using the index than the main table
     - Global Secondary Index
       - can create when you create your table, or add it later
       - different partition key as well as different sort key, gives a completely different view of the data
       - speeds up any queries relating to this alternative partition key and sort key
Scan versus Query API Call
  - Two different ways of getting data out of a DynamoDB table:
    - Query
      - finds items in the table based on the primary key and a distinct value to search for
      - can use an optional sort key name and value to refine the results
      - by default returns all the attributes for the items selected but you can use the ProjectionExpression parameter if you want to only return specific attributes
      - results are always sorted by the sort key
      - numeric order by default is ascending
      - ASCII character values ordered in ascending or by default
      - you can reverse the order by setting the ScanIndexForward parameter to false
      - eventually consistent by default
      - need to explicitly set to strongly consistent
    - Scan
      - examines every item in the table
      - by default returns all data attributes
      - use the ProjectExpression parameter to only return the attributes you want
      - can filter the results once it has been run
  - Query or Scan
    - query is more efficient than scan
    - scan dumps the entire table, then filters out the values to provide the desired result - removing the unwanted data
    - a scan on a large table can use up the provisioned throughput for a large table in just a single operation
    - you can reduce the impact of a query or scan by setting a smaller page size which uses fewer read operations
      - larger numbers of smaller read operations will allow other requests to succeed without throttling
    - avoid using scan operations if you can
      - design tables in a way that you can use the Query, Get, or BatchGetItem APIs
    - by default, a scan operation processes data sequentially in returning 1MB increments before moving on to retrieve the next 1MB of data
      - can only scan 1 partition at a time
    - you can configure DynamoDB to use Parallel scans instead by logically dividing a table or index into segments and scanning each segment in parallel
      - it is best to avoid parallel scans if your table or index is already incurring heavy read - write activity from other applications
DynamoDB Provisioned Throughput
  - DynamoDB provisioned throughput is measured in Capacity Units
  - When you create a table, you specify your requirements in terms of Read Capacity Units and Write Capacity Units
  - 1 x write capacity unit = 1 x 1KB write per second
  - 1 x read capacity unit = 1 x strongly consistent read of 4KB per second OR 2 x eventually consistent reads of 4KB per second (default)
  - If your application reads or writes larger items it will consume more capacity units and will cost you more
DynamoDB On-Demand Capacity
  - Charges apply for: reading, writing, and storing data
  - With On-Demand, you do not need to specify your requirements
  - DynamoDB instantly scales up and down based on the activity of your application
  - Great for unpredictable workloads
  - You pay only for what you use, pay per request
  - On-Demand good for:
    - unknown workloads
    - unpredictable application traffic
    - pay-per-use model
    - spiky, short-lived peaks
  - Provisioned-Capacity good for:
    - Forecastable read and write capacity requirements
    - predicable application traffic
    - application traffic is consistent or increases gradually
  - Can change the pricing model once per day
DynamoDB Accelerator (DAX)
  - DynamoDB Accelerator (DAX) is a fully managed, clustered in-memory cache for DynamoDB
  - Delivers up to 10x read performance improvement
  - Microsecond performance for millions of requests per second
  - Ideal for read-heavy and bursty read workloads
    - auction applications
    - gaming
    - retail sites during Black Friday promotions
  - DAX is a write-through caching service
    - data is written to the cache as well as the back-end store at the same time
  - Allows you to point your DynamoDB API calls at the DAX cluster
  - If the item you are querying is in the cache (a cache hit), DAX returns the cached result to the application
  - If the item is not in the cache (cache miss) then DAX performs an eventually consistent GetItem operation against the DynamoDB table
  - Retrieval of data from DAX reduces the read load on DynamoDB tables
  - May be able to reduce provisioned read capacity
  - DAX improves response time for Eventually Consistent reads only
  - Not suitable for DAX
    - applications that require strongly consistent reads
    - write intensive applications
    - applications that do not perform many read operations
    - applications that do not require microsecond response time
  - Point your API calls at the DAX cluster instead of your DynamoDB table
ElastiCache (START HERE)
  -
