S3 101
  - S3 provides developers and IT teams with secure, durable, highly-scalable object storage
  - S3 has a simple web service interface to store and retrieve any amount of data from anywhere on the web
  - S3 is not for operating systems or Databases
  - S3 is a safe place to store your files
  - S3 is object storage based, not block storage based
  - With S3, your data is spread across multiple devices and facilities
  - S3 is object based - i.e. allows you to upload files
  - S3 files can be from 0 Bytes to 5 TB
  - S3 has unlimited storage
  - With S3, files are stored in buckets, which are similar to folders
  - S3 has a universal name space, that is, bucket names must be unique globally; similar to a DNS address
  - When you upload a file into S3 you will receive an HTTP 200 code if the upload was successful
  - S3 Data Consistency: two modes
    - Read after Write consistency for PUTS of new objects
    - Eventual consistency for overwrite PUTS and DELETES (can take some time to propagate)
  - S3 is a simple key-value store
    - S3 is object based; objects consist of the following:
      - a Key, which is simply the name of the object
      - a Value, which is simply the data, which is made up of a series of bytes
      - a Version ID, which is important for versioning
      - Metadata, which is data about the data you are storing
      - Subresources, which are bucket-specific configuration:
        - Bucket policies
        - Access Control Lists
        - Cross Origin Resource Sharing (CORS); files located in one bucket to access files located in another bucket
        - Transfer Acceleration, which is a service which allows you to accelerate file transfers when upload many files into S3
  - S3 is built for 99.99% availability
  - Amazon guarantees 99.9% availability
  - Amazon guarantees 99.999999999% durability for S3 information (11 9s)
  - S3 has tiered storage available
  - S3 has lifecycle management
  - S3 has versioning
  - S3 has encryption
  - You can secure access to your data using:
    - Access Control Lists
    - Bucket Policies
  - S3 Storage Tiers
    - Regular S3
      - 99.99% availability, 99.999999999% durability, stored redundantly across multiple devices in multiple facilities;
        designed to sustain a loss of 2 facilities concurrently
    - S3 IA (Infrequently Accessed)
      - For data that is accessed less frequently, but requires rapid access when needed
      - Lower fee than S3, but you are charged a retrieval fee every time you retrieve the data
    - One-Zone IA
      - Same as IA however data is stored in a single AZ only
      - Still 99.999999999% durability
      - Only 99.5% availability
      - Costs 20% less than regular IA
    - Reduced Redundancy Storage
      - Designed to provide 99.99% durability and 99.99% availability of objects over a given year
      - Used for data that can be recreated if lost, e.g. thumbnails
    - Glacier
      - Very inexpensive, but used for archival only
      - Optimized for data that is infrequently accessed
      - It takes 3 to 5 hours to restore data from Glacier
    - S3 Intelligent Tiering
      - Suitable for data which has unknown or unpredictable access patterns
      - 2 Tiers
        - frequent access
        - infrequent access
      - Automatically moves your data to the most cost-effective tier based on how frequently you access each object within a bucket
      - If an object is not accessed for 30 consecutive days, it gets automatically moved to the infrequent access tier
      - As soon as an object in the infrequent access tier is accessed, it is moved to the frequent access tier
      - 99.99999999999% durability
      - 99.9% availability over a given year
      - Helps optimize cost
      - No fees for accessing your data but a small monthly fee for monitoring/automation $0.0025 per 1,000 objects
 - S3 Charges
   - Storage per GB
   - Requests (Get, Put, Copy, etc.)
   - Storage Management pricing
     - Inventory, Analytics, and Object Tags
   - Data management pricing
     - Data transferred out of S3
     - Free to transfer in to S3
   - Transfer Acceleration
     - Use of CloudFront to optimize transfers
 - Exam Tips
   - Example S3 bucket url: https://s3-eu-west-1.amazonaws.com/bucketname
S3 Security
 - By default, all newly created buckets are private
 - You can setup access control to your buckets using:
   - Bucket Policies: applied at the bucket level
   - Access Control Lists: applied at the object level
 - S3 buckets can be configured to create access logs, which log all requests made to the S3 bucket. These logs can be written to another bucket
 - Bucket policies are written in JSON
 - All versions of a versioned S3 bucket are kept in the same bucket
 - S3 object-level logging records any api level activity using AWS CloudTrail, at an additional cost
 - S3 has two different types of encryption:
   - AES-256: uses server-side encryption with AWS S3-Managed Keys (SSE-S3)
   - AWS-KMS: uses server-side encryption with AWS KMS-Managed Keys (SSE-KMS)
 - You can enable S3 CloudWatch request metrics, which monitor requests in your bucket for an additional cost
   - gives performance metrics for S3
 - S3 has public access settings
   - Used to enforce that buckets do not allow public access to data.
   - You can also configure S3 to block public access at the account level.
 - You can grant public read-access to objects in the Access Control List
 - You can encrypt at rest at the bucket level using bucket policies or the object level using access control lists
 - If the bucket policy does not allow public access, you cannot upload an object into the bucket and make that object publicly accessible using the Access Control List
S3 Encryption
 - In transit
   - SSL/TLS (Transport Layer Security, will replace SSL)
     - generally means using https to transmit data
 - At Rest
   - Server-side encryption - 3 types
     - S3 managed keys - SSE-S3
       - AWS manages the keys for you, rotate keys on a defined frequency
       - uses AES-256 bit encryption
     - AWS Key Management Service, managed keys - SSE-KMS
       - AWS manages the keys for you
       - uses an envelope key that encrypts your encryption key
       - gives an audit trail which records the use of your encryption key
       - can use your own key, or the default AWS key
     - Server-side encryption with customer provided keys - SSE-C
       - AWS manages the encryption and decryption but you manage your own keys
 - Client-side encryption
   - you encrypt the files yourself before you upload to S3
 - Enforcement of S3 Encryption
   - Every time a file is uploaded to S3, a PUT request is initiated
   - If the file is to be encrypted at upload time, the x-amz-server-side-encryption parameter will be included in the request header
     - two options are currently available
       - x-amz-server-side-encryption: AES256 (SSE-S3 - S3 managed keys)
       - x-amz-server-side-encryption: ams:kms (SSE-KMS - KMS managed keys)
     - when this parameter is included in the header of the PUT request, it tells S3 to encrypt the object at the time of upload, using the specified encryption method
     - you can enforce the use of server-side encryption by using a bucket policy which denies any S3 PUT request which does not include the x-amz-server-side-encryption
       parameter in the request header
CloudFront
 - A Content Delivery Network (CDN) is a system of distributed servers (network) that deliver webpages and other web content to a user based on the:
   - geographic location of the user
   - the origin of the web page
   - a content delivery server
 - Edge Locations are collections of servers in dispersed geographic locations
   - used by CloudFront to cache web content close to geographically dispersed users
   - can write content into Edge Locations (PUT an object to them) as well as read from them
   - separate from AZs or Regions
 - Origin
   - the origin of all the files that the CDN will distribute
   - Origins can be S3 buckets, EC2 instances, an ELB, or Route53
 - Distribution
   - the name given to the CDN
   - consists of a collection of edge locations
   - Two types of distributions
     - Web Distribution
       - Typically used for websites
     - RTMP (Real Time Messaging Protocol)
       - Used for media streaming
       - Adobe Flash, multi-media content
 - CloudFront also works with any non-AWS origin server
 - CloudFront accelerates S3 Transfer Acceleration
   - S3 Transfer Acceleration takes advantage of CloudFront's globally distributed edge locations
   - When the data being transferred arrives at an edge location the remaining transfer moves over AWS's optimized network path
 - You can clear cached objects from your CloudFront edge locations but you will be charged
 - Can have multiple origins for a distribution
 - You can clear your CloudFront cache by invalidating objects
S3 Performance Optimization (START HERE)
 - S3 is designed to support very high request rates
   - If your S3 buckets are routinely receiving > 100 PUT/LIST/DELETE or > 300 GET requests per second, then there are some best practice
     guidelines that will help optimize S3 Performance
   - The guidance is based on the type of workload you are running
     - GET-intensive workloads
       - use CloudFront content delivery service to get best performance
       - CloudFront will cache your most frequently accessed objects and will reduce latency for your GET requests
     - Mixed Request Type Workloads
       - a mix of GET, PUT, DELETE, GET Bucket - the key names you use for your objects can impact performance workloads
       - S3 uses the key name to determine which partition an object will be stored in
       - The use of sequential key names e.g. names prefixed with a timestamp or alphabetical sequence increases the likelihood
         of having multiple objects stored on the same partition
       - For heavy workloads this can cause I/O issues and contention
       - By using a random prefix to key names (like a hex hash), you can force S3 to distribute your keys across multiple partitions, distributing the I/O workload
Performance Update
 - S3 can support at least 3,500 put requests per second
 - S3 can support at least 5,500 get requests per second
 - Now you no longer need to randomized key names to achieve faster performance
 - Logical and sequential naming patterns can now be used without any performance implication
S3 Summary
 - S3 website url format http://bucketname.s3-website-regionname.amazonaws.com
S3 Quiz
 - S3 provides unlimited storage
   - true (correct)
   - false
 - What is the HTTP code you would see once you successfully place a file in an S3 bucket?
   - 524
   - 200 (correct)
   - 312
   - 404
 - You are hosting a static website in an S3 bucket that uses Java script to reference assets in another S3 bucket. For some reason, these assets are not displaying when users browse to the site. What could be the problem?
   - You haven not enabled Cross Origin Resource Sharing (CORS) on the bucket where the assets are stored (correct)
   - Amazon S3 does not support Javascript
   - You cannot use one S3 bucket to reference another S3 bucket
   - You need to open port 80 on the appropriate security group in which the S3 bucket is located
 - The minimum file size allowed on S3 is 1 byte
   - true
   - false (correct)
 - How does S3 determine which partition to use to store files?
   - The bucket name determines which partition the file is stored in
   - S3 automatically stores your files on a random partition
   - The key name determines which partition the file is stored in (correct)
   - By default, all files in the same bucket are stored on the same partition
 - Your application is consistently reading and writing 100s of objects per second to S3 and your workload is steadily rising. What can you do to achieve the best performance from S3?
   - Configure an additional bucket and distribute the files evenly between the two buckets
   - Configure a CloudFront CDN and use the S3 bucket as the origin
   - Add a hex hash prefix to the objects key name (correct)
   - Add a hex hash suffix to the objects key name
 - What is the maximum file size that can be stored on S3?
   - 4TB
   - 2TB
   - 1TB
   - 5TB (correct)
 - You are using S3 in AP-Northeast to host a static website in a bucket called acloudguru. What would the new URL endpoint be?
   - http://acloudguru.s3-website-ap-southeast-1.amazonaws.com
   - https://s3-ap-northeast-1.amazonaws.com/acloudguru/
   - http://acloudguru.s3-website-ap-northeast-1.amazonaws.com (correct)
   - http://www.acloudguru.s3-website-ap-northeast-1.amazonaws.com
 - When you first create an S3 bucket, this bucket is publicly accessible by default
   - true
   - false (correct)
 - Which feature of AWS can you use to configure S3 to allow one S3 bucket to access files in another S3 bucket?
   - IAM Role
   - CORS (correct)
   - Bucket ACL
   - Bucket Policy
 - If you want to enable a user to download your private data directly from S3, you can insert a pre-signed URL into a web page before giving it to your user.
   - true (correct)
   - false
 - Which of the following encryption methods are supported in S3? (Choose 3)
   - SSE-C (correct)
   - SSE-AES
   - SSE-KMS (correct)
   - SSE-S3 (correct)
 - What is the largest size file you can transfer to S3 using a PUT operation?
   - 1GB
   - 5GB (correct)
   - 100MB
   - 5TB
 - If you encrypt a bucket on S3, what type of encryption does AWS use?
   - International Data Encryption Algorithm (IDEA)
   - Data Encryption Standard (DES)
   - Advanced Encryption Standard (AES) 256 (correct)
   - Advanced Encryption Standard (AES) 128
 - Which of the following options allows users to have secure access to private files located in S3? (Choose 3)
   - CloudFront Origin Access Identity (correct)
   - Public S3 buckets
   - CloudFront Signed URLs (correct)
   - CloudFront Signed Cookies (correct)
Serverless 101

Lambda
  - Lambda is a compute service where you can upload your code and create a lambda function
  - AWS lambda takes care of provisioning and managing the servers that you use to run the code
  - You don't have to worry about operating systems, patching, scaling, etc.
  - You can use lambda in the following ways:
     - As an event-driven compute service where lambda runs your code in response to events
       - These events could be changes to data in an S3 bucket or an DynamoDB table
     - As a compute service to run your code in response to HTTP requests using API Gateway or API calls using AWS SDKs.
  - Lambda code can be written in these languages:
    - Node.js
    - Java
    - Python
    - C#
    - Go
  - Lambda is priced as follows:
    - Number of requests
      - first 1 million requests are free per month
      - $0.20 per 1 million requests after the first 1 million requests
    - Duration
      - Duration is calculated from the time your code begins executing until it returns or otherwise terminates, rounded up to the nearest 100ms
      - The price depends on the amount of memory you allocate to your function
      - You are charged $0.00001667 for every GB-second used
  - Lambda scales out (not up) automatically
  - AWS X-ray allows you to debug lambda function chains
  - Lambda can be used globally
    - you can use a lambda function to back up S3 buckets across regions
API Gateway
 - API Gateway is a fully managed service that lets developers publish, maintain, monitor, and secure APIs at any scale
 - Can use API Gateway to create APIs that access application code running on EC2 instances, code running in Lambda, or any other web application
 - API Gateway exposes HTTPS endpoints to define a RESTful API
 - API Gateway serverless-ly connects to services like Lambda & DynamoDB
 - API Gateway can send each API endpoint to a different target
 - Runs efficiently with low cost
 - Scales effortlessly
 - Track and control usage by API key
 - Throttle requests to prevent attacks
 - Connect to CloudWatch to log all requests for monitoring
 - Maintain multiple versions of your API
 - Defining an API in API Gateway:
   - Define an API (container)
   - Define resources and nested resources (URL paths)
     - For each resource:
       - Select supported HTTP methods (verbs)
       - Set security
       - Choose target (such as EC2, Lambda, DynamoDB, etc.)
       - Set request and response transformations
   - Deploy API to a stage
     - Uses API Gateway domain, by default
     - Can use a custom domain
     - Now supports AWS Certificate Manager: free SSL/TLS certs
 - Can enable API caching
   - Cache your endpoint's response
 - Same-origin-policy
   - allows scripts in a first web page to access data from a second web page only if both pages are served from the same origin
   - used to prevent cross-site scripting
   - enforced by web browsers
 - Cross-Origin-Resource-Sharing (CORS)
   - relaxes the same-origin-policy
   - Allows resources in a web page to be requested from a domain which is different from the domain which served the web page
   - the browser makes an HTTP OPTIONS call for a URL
   - The server returns a response that lists the domains approved for access from the URL
   - Or the server returns an error stating that the origin policy cannot be read at the remote resource, enable CORS on API Gateway
 - Lambda trigger services:
   - API Gateway
   - AWS IoT
   - Alexa Skills Kit
   - Alexa Smart Home
   - CloudFront
   - CloudWatch Events
   - CloudWatch Logs
   - CodeCommit
   - Cognito Sync Trigger
   - DynamoDB
   - Kinesis
   - S3
   - SNS
 - API Gateway integration point types
   - HTTP
   - Lambda Function
   - Mock: return a response purely using API Gateway mappings and transformations
   - AWS Service
   - VPC Link
Version Control with Lambda
   - When you use versioning with Lambda, you can publish one or more versions of your Lambda function
   - Each version of your Lambda function has a unique ARN
   - After you publish a version it is immutable
   - There are two types of ARNs associated with a Lambda function
     - Qualified ARN - The function ARN with the version suffix
       - arn:aws:lambda:aws-region:acct-id:function:hwlloworld:$LATEST
     - Unqualified ARN - The function ARN without the suffix
       - arn:aws:lambda:aws-region:acct-id:function:hwlloworld
   - Alias
     - After publishing your Lambda function, then create an alias to it, such as PROD, you can then use the PROD alias to invoke this version of the function
     - If you make changes and publish this new version, you can point the PROD alias to the new version.
     - If you decide to roll back to the old version, you just repoint the alias to the old version
   - You can use versioning to create splits between traffic, such as blue/green deployments
     - You cannot use $LATEST in version splitting
Step Functions
   - Step Functions allow you to visualize and test your serverless applications
   - Provides a graphical console to arrange and visualize the components of your application as a series of steps
   - Automatically triggers and tracks each step, and retries when there are errors
   - Logs the state of each step, so you can diagnose and debug
   - Can have sequential steps
   - Can have branching steps
   - Can have parallel steps
   - Use Amazon States Language to create steps
X-Ray
   - X-Ray is a service that collects data about requests that your application serves, and provides tools you can use to view, filter, and gain insights into that data
     to identify issues and opportunities for optimization
   - For any traced request to your application, you can see details of the request and response, as well as calls your application makes to downstream
     AWS resources, microservices, databases, and HTTP web APIs
   - Use the X-Ray SDK inside your application
      - the X-Ray SDK sends information (in JSON) to the X-Ray Daemon, which sends the JSON to the X-Ray API
      - the X-Ray creates a visualization of the functioning of your app, which it displays in the X-Ray Console
      - the AWS SDK and the AWS CLI can communicate directly with the X-Ray Daemon and/or the X-Ray API
   - The X-Ray SDK provides
      - Interceptors to add to your code to trace incoming HTTP requests
      - Client handlers to instrument AWS SDK clients that your application uses to call other AWS services
      - An HTTP client to use to instrument calls to other internal and external HTTP web services
   - X-Ray integrates with the following AWS services
      - Elastic Load Balancing
      - Lambda
      - API Gateway
      - EC2
      - Elastic Beanstalk
   - X-Ray supported languages:
      - Java
      - Go
      - Node.js
      - Python
      - Ruby
      - .Net
Advanced API Gateway
  - You can use the API Gateway Import API feature to import an API from an external definition file into API Gateway
  - Currently the API feature supports Swagger v2.0 definition files
  - With the Import API, you can either create a new API by submitting a POST request that includes a Swagger definition in the payload and endpoint configuration,
    or you can update an existing API by using a PUT request that contains a Swagger definition in the payload
  - You can update an API by overwriting it with a new definition, or merge a definition with an existing API
  - You specify the options using a mode query parameter in the request URL
  - API Throttling
    - by default API Gateway limits the steady-state request rate to 10,000 requests per second (rps)
    - The maximum concurrent requests is 5,000 requests across all APIs within an AWS account
    - If you go over 10,000 requests per second or over 5,000 concurrent requests you will receive a 429 Too Many Requests error response
  - SOAP Webservice Passthrough
    - You can configure API Gateway as a SOAP web service passthrough
Serverless Quiz
  - Which of the following services does X-ray integrate with? (Choose 3)
    - Lambda (correct)
    - S3
    - Elastic Load Balancer (correct)
    - API Gateway (correct)
 - You have launched a new web application on AWS using API Gateway, Lambda and S3. Someone posts a thread to reddit about your application and it starts to go viral. You start receiving 10,000 requests every second and you notice that most requests are similar. Your web application begins to struggle. What can you do to optimize performance of your application?
   - Enable API Gateway caching to cache frequent requests (correct)
   - Enable API Gateway Accelerator
   - Migrate your API Gateway to a Network Load Balancer and enable session stickiness for all sessions
   - Change your Route53 alias record to point to AWS Neptune and then configure Neptune to filter your API requests to genuine requests only
 - You have created an application using serverless architecture using Lambda, Api Gateway, S3 and DynamoDB. Your boss asks you to do a major upgrade to API Gateway and you do this and deploy it to production. Unfortunately something has gone wrong and now your application is offline. What should you do to bring your application up as quickly as possible?
   - Restore your previous API gateway configuration using an EBS snapshot
   - Rollback your API Gateway to the previous stage (correct)
   - Restart API Gateway for the new changes to take effect
   - Delete the existing API Gateway
 - You have created a serverless application which converts text into speech using a combination of S3, API Gateway, Lambda, Polly, DynamoDB and SNS. Your users complain that only some text is being converted, whereas longer amounts of text does not get converted. What could be the cause of this problem?
   - Polly has built in censorship, so if you try and send it text that is deemed offensive, it will not generate an MP3
   - You have placed your DynamoDB table in a single availability zone which is currently down, causing an outage
   - AWS X-ray service is interfering with the application and should be disabled
   - Your lambda function needs a longer execution time. You should check how long is needed in the fringe cases and increase the timeout inside the function to slightly longer than that (correct)
 - How does API Gateway deal with legacy SOAP applications
   - Converts the response from the application to XML
   - Provides webservice passthrough for SOAP applications (correct)
   - Converts the response from the application to REST
   - Converts the response from the application to HTML
 - You work for a gaming company that has built a serverless application on AWS using Lambda, API Gateway and DynamoDB. They release a new version of the lambda function and the application stops working. You need to get the application up and back online as fast as possible. What should you do?
   - DynamoDB is not serverless and is causing the error. Migrate your database to RDS and redeploy the lambda function
   - Create a CloudFormation template of the environment. Deploy this template to a separate region and then redirect Route53 to the new region
   - The new function has some dependencies not available to lambda. Redeploy the application on EC2 and put the EC2 instances behind a network load balancer
   - Roll your lambda function back to the previous version (correct)
 - You are developing a new application using serverless infrastructure and are using services such as S3, DynamoDB, Lambda, API Gateway, CloudFront, CloudFormation and Polly. You deploy your application to production and your end users begin complaining about receiving a HTTP 429 error. What could be the cause of the error?
   - Your cloudformation stack is not valid and is failing to deploy properly which is causing a HTTP 429 error
   - You enabled API throttling for a rate limit of 1000 requests per second while in development and now that you have deployed to production your API Gateway is being throttled (correct)
   - You have an S3 bucket policy which is preventing lambda from being able to write files to your bucket, generating a HTTP 429 error
   - Your lambda function does not have sufficient permissions to read to DynamoDB and this is generating a HTTP 429 error
 - You are a developer for a busy real estate company and you want to enable other real estate agents to the ability to show properties on your books, but skinned so that it looks like their own website. You decide the most efficient way to do this is to create is to expose your API to the public. The project works well, however one of your competitors starts abusing this, sending your API tens of thousands of requests per second. This generates a HTTP 429 error. Each agent connects to your API using individual API keys. What actions can you take to stop this behavior?
   - Use AWS Shield Advanced API protection to block the requests
   - Place an AWS Web Application Firewall in front of API gateway and filter the requests
   - Deploy multiple API Gateways and give the agent access to another API Gateway
   - Throttle the agents API access using the individual API Keys (correct)
 - You have designed a serverless learning management system and you are ready to deploy your code in to a test-dev environment. The system uses a combination of Lambda, API Gateway, S3 and CloudFront and is architected to be highly available, fault tolerant and scalable. What are the three different ways you can deploy your code to lambda. (Choose 3)
   - Zip your code into a zip file and upload it via the Lambda console (correct)
   - Import your code to Amazonâ€™s Glacier and change your Lambda execution handler to point to glacier
   - Copy and paste your code in to the integrated development environment (IDE) inside Lambda (correct)
   - Upload your code to S3 and use a MySQL connection string to connect Lambda to your S3 bucket
   - Write a CloudFormation template that will deploy your environment including your code (correct)
 - You have created a simple serverless website using S3, Lambda, API Gateway and DynamoDB. Your website will process the contact details of your customers, predict an expected delivery date of their order and store their order in DynamoDB. You test the website before deploying it into production and you notice that although the page executes, and the lambda function is triggered, it is unable to write to DynamoDB. What could be the cause of this issue?
   - The availability zone that DynamoDB is hosted in is down
   - You have written your function in python which is not supported as a run time environment for Lambda
   - Your lambda function does not have the sufficient Identity Access Management (IAM) permissions to write to DynamoDB (correct)
   - The availability zone that Lambda is hosted in is down
 - You have an internal API that you use for your corporate network. Your company has decided to go all in on AWS to reduce their data center footprint. They will need to leverage their existing API within AWS. What is the most efficient way to do this?
   - Use AWS API Import-Export feature of AWS Storage Gateway
   - Recreate the API manually
   - Use the Swagger Importer tool to import your API in to API Gateway (correct)
   - Replicate your API to API Gateway using the API Replication Master
Introduction to DynamoDB
 - DynamoDB supports both document and key-value data models
 - It is a great fit for mobile, web, gaming, ad-tech, IoT, and many others
 - DynamoDB is serverless
 - Fully Managed
 - Can be configured to auto-scale
 - DynamoDB tables are stored on SSD storage
 - Underlying hardware supporting your DynamoDB tables is spread across 3 geographically distinct datacenters
 - DynamoDB has a choice of two consistency models:
   - Eventually consistent reads (default)
     - Consistency across all the copies of data is usually reached within a second
     - Repeating a read after a short time should return the updated data (best performance)
   - Strongly consistent reads
     - A strongly consistent read returns a result that reflects all writes that received a successful response prior to the read
 - DynamoDB is made of of:
   - Tables
   - Items (think of a row of data)
   - Attributes (think of a column of data)
   - Supports key-value and document data structures
   - Key = the name of the data, Value = the data itself
   - Documents can be written in JSON, XML, or HTML
 - DynamoDB stores and retrieves data based on a Primary Key
 - 2 types of primary key:
   - Partition Key: unique attribute (i.e. userID)
     - The value of the Partition Key is input into an internal hash function which determines the partition or physical location on which the data is stored
     - If you use the Partition Key as your Primary Key, then no two items can have the same Partition Key
   - Composite Key: Partition Key + Sort Key
     - The primary key is a composite key which is a consisting of:
       - a partition key (i.e. userID)
       - a sort key (i.e. the timestamp of the post)
       - 2 items may have the same partition key but the must have different different sort keys
       - All items with the same partition key are stored together then sorted according to the sort key
       - Allows you to store multiple items with the same partition key
 - Access Control to DynamoDB
   - Authentication and access control is managed by IAM policies
   - You can create an IAM user which has specific permissions to access and create DynamoDB tables
   - You can create an IAM role which allows you to obtain temporary access keys which can be used to access DynamoDB
   - You can use a special IAM Condition to an IAM policy to restrict user access to only their own records based on their partition key (i.e. userID)
     - the dynamodb:LeadingKeys parameter in the policy is used to allow users to access only the items where where the partition key value matches their userID
DynamoDB Indexes - Deep Dive
   - DynamoDB supports 2 types of indexes to speed up queries:
     - Local Secondary Index
       - can only be created when you are creating your table
       - you cannot add, remove, or modify it once it has been created
       - it has the same partition key as the table it indexes
       - it has a different sort key than the table it indexes
       - gives you a different view of your data, organized according to an alternative sort key
       - any queries based on this index are much faster using the index than the main table
     - Global Secondary Index
       - can create when you create your table, or add it later
       - different partition key as well as different sort key, gives a completely different view of the data
       - speeds up any queries relating to this alternative partition key and sort key
Scan versus Query API Call
  - Two different ways of getting data out of a DynamoDB table:
    - Query
      - finds items in the table based on the primary key and a distinct value to search for
      - can use an optional sort key name and value to refine the results
      - by default returns all the attributes for the items selected but you can use the ProjectionExpression parameter if you want to only return specific attributes
      - results are always sorted by the sort key
      - numeric order by default is ascending
      - ASCII character values ordered in ascending or by default
      - you can reverse the order by setting the ScanIndexForward parameter to false
      - eventually consistent by default
      - need to explicitly set to strongly consistent
    - Scan
      - examines every item in the table
      - by default returns all data attributes
      - use the ProjectExpression parameter to only return the attributes you want
      - can filter the results once it has been run
  - Query or Scan
    - query is more efficient than scan
    - scan dumps the entire table, then filters out the values to provide the desired result - removing the unwanted data
    - a scan on a large table can use up the provisioned throughput for a large table in just a single operation
    - you can reduce the impact of a query or scan by setting a smaller page size which uses fewer read operations
      - larger numbers of smaller read operations will allow other requests to succeed without throttling
    - avoid using scan operations if you can
      - design tables in a way that you can use the Query, Get, or BatchGetItem APIs
    - by default, a scan operation processes data sequentially in returning 1MB increments before moving on to retrieve the next 1MB of data
      - can only scan 1 partition at a time
    - you can configure DynamoDB to use Parallel scans instead by logically dividing a table or index into segments and scanning each segment in parallel
      - it is best to avoid parallel scans if your table or index is already incurring heavy read - write activity from other applications
DynamoDB Provisioned Throughput
  - DynamoDB provisioned throughput is measured in Capacity Units
  - When you create a table, you specify your requirements in terms of Read Capacity Units and Write Capacity Units
  - 1 x write capacity unit = 1 x 1KB write per second
  - 1 x read capacity unit = 1 x strongly consistent read of 4KB per second OR 2 x eventually consistent reads of 4KB per second (default)
  - If your application reads or writes larger items it will consume more capacity units and will cost you more
DynamoDB On-Demand Capacity
  - Charges apply for: reading, writing, and storing data
  - With On-Demand, you do not need to specify your requirements
  - DynamoDB instantly scales up and down based on the activity of your application
  - Great for unpredictable workloads
  - You pay only for what you use, pay per request
  - On-Demand good for:
    - unknown workloads
    - unpredictable application traffic
    - pay-per-use model
    - spiky, short-lived peaks
  - Provisioned-Capacity good for:
    - Forecastable read and write capacity requirements
    - predicable application traffic
    - application traffic is consistent or increases gradually
  - Can change the pricing model once per day
DynamoDB Accelerator (DAX)
  - DynamoDB Accelerator (DAX) is a fully managed, clustered in-memory cache for DynamoDB
  - Delivers up to 10x read performance improvement
  - Microsecond performance for millions of requests per second
  - Ideal for read-heavy and bursty read workloads
    - auction applications
    - gaming
    - retail sites during Black Friday promotions
  - DAX is a write-through caching service
    - data is written to the cache as well as the back-end store at the same time
  - Allows you to point your DynamoDB API calls at the DAX cluster
  - If the item you are querying is in the cache (a cache hit), DAX returns the cached result to the application
  - If the item is not in the cache (cache miss) then DAX performs an eventually consistent GetItem operation against the DynamoDB table
  - Retrieval of data from DAX reduces the read load on DynamoDB tables
  - May be able to reduce provisioned read capacity
  - DAX improves response time for Eventually Consistent reads only
  - Not suitable for DAX
    - applications that require strongly consistent reads
    - write intensive applications
    - applications that do not perform many read operations
    - applications that do not require microsecond response time
  - Point your API calls at the DAX cluster instead of your DynamoDB table
ElastiCache
  - In-memory cache in the cloud
  - Sits between application and database
  - Good if your database is particularly read-heavy and the data is not changing frequently
  - Good for read-heavy workloads
    - social networking
    - gaming
    - media sharing
    - Q&A portals
  - Also good for compute-heavy workloads
    - recommendation engines
  - Can be used to store results of I/O intensive database queries or output of compute-intensive calculations
  - Two types of ElastiCache
    - Memcached
      - Widely adopted memory object caching system
      - Multi-threaded
      - No Multi-AZ capability
    - Redis
      - Open-source in-memory key-value store
      - Supports more complicated data structures
        - sorted sets
        - lists
      - Supports master-slave replication and Multi-AZ for cross-AZ redundancy
  - Two caching strategies
    - Lazy Loading
      - loads the data into the cache only when necessary
      - if the requested data is in the cache, ElastiCache returns the data to the application
      - if the data is not in the cache or has expired, ElastiCache returns a null
        - your application then feteches the data from the database and writes the retrieved data to the cache
      - Advantages
        - Only requested data is cached, which avoids filling up the cache with useless data
        - Node failures are not fatal, a new empty node will just have a lot of cache misses initially
      - Disadvantages
        - Cache miss penalty: initial request query to database then writing of data to the cache
        - Stale data: if data is only updated when there is a cache miss, it can become stale; does not automatically update if the data in the database changes
      - Lazy Loading and TTL
        - TTL specifies the number of seconds until the key (data) expires to avoid keeping stale data in the cache
        - Lazy Loading treats an expired key as a cache miss and causes the application to retrieve the data from the database and subsequently write the data into the cache with a new TTL
        - TTL does not eliminate stale data but helps to avoid it
    - Write-Through
      - adds or updates data to the cache whenever data is written to the database
      - Advantages
        - Data in the cache is never stale
        - Users are generally more tolerant of additional latency when updating data than when retrieving it
      - Disadvantages
        - Write penalty: every write involves write involves a write to the cache as well as a write to the database
        - if a node fails and a new one is spun up, data is missing until added or updated in the database (mitigate by implementing lazy loading in conjunction with write-through)
        - wasted resources if most of the data is never read
  - DAX written specifically for DynamoDB and DAX only supports the write-through caching strategy
  - If you are using DynamoDB and you need to use lazy loading, you cannot use DAX, you have to use ElastiCache
  - DAX for DynamoDB, ElastiCache for RDS
DynamoDB Transactions, introduced at re:Invent 2018
  - ACID transactions (Atomic, Consistent, Isolated, Durable)
  - read or write multiple items across multiple tables as an all or nothing operation
  - check for a prerequisite condition before writing to table
DynamoDB TTL
  - DynamoDB TTL defines an expiry time for your data
  - Expired items marked for deletion
  - Great for removing irrelevant old data:
    - session data
    - event logs
    - temporary data
  - Reduces cost by automatically removing old data which is no longer relevant
  - TTL is expressed in epoch time format, aka unix time, aka posix time (number of seconds that have elapsed sine 12:00 AM Jan 1 1970)
  - Items are expired when the current time exceeds the TTL time value
  - Items are deleted within the next 48 hours after the TTL expires
  - You can filter out expired items from your queries and scans
  - DynamoDB refers to the primary key as the HASH key when using the CLI
  - Select an attribute in your table which holds the TTL value
DynamoDB Streams
  - DynamoDB Streams are a time-ordered sequence of item level modifications (insert, update, delete)
  - logs are encrypted at rest and stored for 24 hours only
  - accessed using a dedicated endpoint, different from the endpoint used to access the DynamoDB table itself
  - by default the primary key is recorded
  - before and after images can be captured
    - store state of the item before the change and the state of the item after the change
  - can be used for auditing, archiving, replaying transactions to a different table
  - mainly used to trigger events based on a change in the DynamoDB table
    - can use a DynamDB stream to trigger a lambda function
  - events are recorded in near real-time
  - applications can take action based on the contents of the stream
  - event source for Lambda
    - lambda polls the stream
    - executes lambda code based on a streams event
Provisioned Throughput Exceeded and Exponential Backoff
  - The Provisioned Throughput Exceeded Exception event is ProvisionedThroughputExceededException
  - this happens when your request rate is too high for the read-write capacity provisioned on your DynamoDB table
  - SDK will automatically retry the requests until successful
  - If you are not using the SDK you can
    - reduce the request frequency
    - use Exponential Backoff
  - Exponential Backoff uses progressively longer waits between consecutive retries
  - If after one minute exponential backoff does not work, your request size may be exceeding the throughput for your read-write capacity
    - if your application is mainly using GETs try using DAX or ElastiCache
    - if your application is mainly doing writes try increasing your write throughput capacity
  - All AWS SDKs and many AWS services (S3, CloudFormation, SES) use exponential backoff
DynamoDB Test Questions
  - Which DynamoDB feature can be used to define an expiry date and time for a data record in your table?
    - Exponential Backoff
    - TTL (correct)
    - DynamoDB Expiry
    - DynamoDB Streams
  - DynamoDB is a No-SQL database provided by AWS
    - true (correct)
    - false
  - What does the error ProvisionedThroughputExceededException mean in DynamoDB?
    - The DynamoDB table has exceeded the allocated space
    - You exceeded your maximum allowed provisioned throughput for a table or for one or more global secondary indexes (correct)
    - Your EC2 instance has run out of CPU or memory
    - The DynamoDB table is unavailable
  - You have an application which reads 80 items of data every second. Each item consists of 3KB. Your application uses eventually consistent reads. What should you set the RCU read throughput to?
    - 80 RCU
    - 20 RCU
    - 60 RCU
    - 40 RCU (correct)
  - Which of the following attributes would make a good Partition Key?
    - WarehouseLocation
    - ProductType
    - Size
    - ProductID (correct)
  - Which of the following attributes would make a good Sort Key?
    - OrderNumber
    - CustomerID
    - EmailAddress
    - InvoiceDate (correct)
  - You are running a query on your Customers table in DynamoBD, however you only want the query to return CustomerID and EmailAddress for each item in the table, how can you refine the query so that it only includes the required attributes?
    - Use the ProjectionExpression parameter (correct)
    - Run a query based on the Primary Key and filter the results using a Sort Key of EmailAddress
    - Use a scan operation instead
    - Create a Global Secondary Index which only includes the attributes you need and run the query on the Global Secondary Index
  - Using the AWS portal, you are trying to Scale DynamoDB past its preconfigured maximums. Which service limit can you increase by raising a ticket to AWS support?
    - Item Sizes
    - Local Secondary Indexes
    - Global Secondary Indexes
    - Provisioned throughput limits (correct)
  - Your DynamoDB table is throwing a ProvisionedThroughputExceeded error, what could be the problem?
    - Your applications request rate is too high (correct)
    - Your instance is too small
    - You are experiencing network contention
    - You are running out of disk space
  - Which feature can you use to capture a time-ordered sequence of all the activity which occurs on your DynamoDB table - e.g. insert, update, delete?
    - DynamoDB Streams (correct)
    - CloudWatch
    - CloudTrail
    - Kinesis Streams
  - In terms of performance, a scan is more efficient than a query
    - true
    - false (correct)
  - By default, a DynamoDB query operation is used for which of the following?
    - Return the entire contents of a table
    - Find items in a table based on the Sort Key attribute
    - Return the entire contents of a table filtered on the Primary Key attribute
    - Find items in a table based on the Primary Key attribute (correct)
  - Which of the following services provides in-memory write-through cache optimized for DynamoDB?
    - DAX (correct)
    - ElastiCache
    - Read-replica
    - CloudFront
  - Your application is storing customer order data in DynamoDB. Which of the following pairs of attributes would make the best composite key to allow you to query DynamoDB efficiently to find a customer order that was placed on a specific day?
    - CustomerID + ProductID
    - CustomerID + OrderDate (correct)
    - CustomerID + Size
    - CustomerID + ProductCategory
  - Which of the following services can be used to securely store confidential information like credentials and license codes so that they can be accessed by EC2 instances?
    - DynamoDB
    - Systems Manager Parameter Store (correct)
    - IAM
    - KMS
  - You have an application that needs to read 25 items of 13KB in size per second. Your application uses eventually consistent reads. What should you set the read throughput to?
    - 100 RCU
    - 10 RCU
    - 50 RCU (correct)
    - 25 RCU
  - What is the API call to retrieve multiple items from a DynamoDB table?
    - BatchGet
    - BatchGetItem (correct)
    - BatchGetItems
    - GetItems
  - What is the difference between a Global Secondary Index and a Local Secondary Index (Choose 2)
    - You can create a Global Secondary Index at any time but you can only create a Local Secondary Index at table creation time (correct)
    - You can delete a Global Secondary Index at any time (correct)
    - You can delete a Local Secondary Index at any time
    - You can create a Local Secondary Index at any time but you can only create a Global Secondary Index at table creation time
  - In DynamoDB, a scan operation is used for which of the following?
    - Find items in a table based on the Primary Key attribute
    - Find items in a table based on the Sort Key attribute
    - Return the entire contents of a table (correct)
    - Return the entire contents of a table filtered on the Primary Key attribute
  - Which of the following approaches is used in AWS to improve flow control by retrying failed requests using progressively longer waits between retries?
    - Backoff With Jitter
    - Linear Backoff
    - Exponential Backoff (correct)
    - Exponential Retry
  - You have a motion sensor which writes 600 items of data every minute. Each item consists of 5KB. What should you set the write throughput to?
    - 20
    - 50 (correct)
    - 10
    - 40
  - Which of the following are ways of remediating a ProvisionedThroughputExceeded error from DynamoDB? (Choose 2)
    - Exponential Backoff (correct)
    - Increase the frequency of requests to the DynamoDB table
    - Reduce the frequency of requests to the DynamoDB table (correct)
    - Move your application to a larger instance type
  - Your application is storing customer order data in a DynamoDB table. You need to run a query to find all the orders placed by a specific customer in the last month, which attributes would you use in your query?
    - A composite Primary Key made up of the CustomerName and OrderDate
    - The Partition Key of OrderDate and a Sort Key of CustomerID
    - The Partition Key of CustomerID and a Sort Key of OrderDate (correct)
    - The Partition Key of OrderDate and Sort Key of CustomerName
  - Your low latency web application needs to store its session state in a scalable way so that it can be accessed quickly. Which service do you recommend?
    - In memory on your EC2 instance
    - Elastic File Store
    - RDS
    - DynamoDB (correct)
KMS 101 (Start Here)
  - KMS is a managed service that makes it easy for you to create and control encryption keys used to encrypt your data
  - KMS is integrated with other AWS services including
    - EBS
    - S3
    - Redshift
    - Elastic Transcoder
    - WorkMail
    - RDS
  - KMS uses encryption keys that you manage
  - KMS encryption keys are regional
  - Usually you have a key administrator (can administer but not use the keys) and a key encrypter (users that can use the key to encrypt and decrypt data), two separate users
  - Customer Master key:
    - CMK consists of:
      - alias
      - creation date
      - description
      - key state (enabled or disabled)
      - key material (either customer provided or AWS provided)
    - can never be exported
    - if you need to export your keys you need to use CloudHSM
  - KMS material options:
    - use KMS generated key material
    - use your own key material, which you import
KMS API Calls
  - Can allow external accounts to use your KMS key to encrypt and decrypt data
  - Cannot have an external account manage-administer the KMS keys
  - to encrypt use aws kms encrypt --key-id YOURKEYID filename
  - to decrypt use aws kms decrypt
  - to re-encrypt use aws kms re-encrypt
  - to rotate keys use aws kms enable-key-rotation to rotate the key every year
KMS Envelope Encryption
  - Envelope encryption is the encryption of your envelope key (which is used to unencrypt your data)
  - use the Customer Master Key to encrypt the envelope key, and the envelope key is what is used to encrypt the data
  - Encrypting the key that encrypts your data
  - Use KMS Master Key to unencrypt the envelop key using an encryption algorithm
  - Customer Master Key used to decrypt the data key (envelope key)
  - Envelope key is used to decrypt the data
KMS Summary
  - To delete a KMS key first disable it, then schedule it for deletion
KMS Quiz Questions
  - You would like KMS to rotate your encryption keys on a yearly basis, which API command can you use to configure this?
    - configure-key-rotation
    - enable-key-rotation (correct)
    - setup-key-rotation
    - chkconfig key-rotation on
  - Which of the following is a managed service that allows you to create and control the encryption keys used to encrypt your data?
    - KMS (correct)
    - RDS Encryption
    - S3 Encryption
    - CMS
  - Which of the following is an encrypted key used by KMS to encrypt your data?
    - Customer Managed Key
    - Customer Master Key
    - Envelope Key (correct)
    - Encryption Key
  - Which of the following statements are correct? (Choose 2)
    - The Envelope Key or Data Key is used to encrypt and decrypt plain text files (correct)
    - The Customer Master Key is used to encrypt and decrypt the Envelope Key or Data Key (correct)
    - The Customer Master Key is used to encrypt and decrypt plain text files
    - The Envelope Key or Data Key is used to encrypt and decrypt the Customer Master Key
  - You are working on a project which requires a Key Management solution. Your security architect has confirmed that a multi-tenant solution is fine. Which solution do you recommend?
    - S3 Encryption
    - CloudHSM
    - KMS (correct)
    - Client Side Encryption
  - Which of the following statements is correct in relation to KMS? (Choose 2)
    - You cannot export your customer master key (correct)
    - KMS encryption keys are regional (correct)
    - KMS encryption keys are global
    - You can export your customer master key
  - Which command can you use to encrypt a plain text file using a CMK?
    - aws iam encrypt
    - aws kms encrypt (correct)
    - aws encrypt
    - aws kms-encrypt
  - You need to re-encrypt a file with a new customer master key, which API call can you use to do this?
    - enable-key-rotation
    - decrypt and encrypt
    - re-encrypt (correct)
    - encrypt
SQS
  - SQS is a web service that gives you access to a distributed queue system, or message queue
  - Can have auto-scaling groups behind SQS so it scales EC2 instances (processors) based on the depth of the queue
  - SQS is a pull based system
  - SNS is a push based system
  - Messages can contain up to 256K of text in any format
  - App code can retrieve the messages using the SQS API
  - Great for when the consuming application code is only intermittently attached to the network
  - Also helps if the producing code produces work faster than the consuming app code can handle
  - Two different types of queues:
    - Standard queue
      - the default queue type
      - nearly unlimited number of transactions per second
      - guarantee that a message is delivered at least once
      - more than one copy might be delivered
      - messages might be delivered out of order
      - provides best-effort ordering
    - FIFO queue
      - messages delivered exactly once
      - order in which messages are sent and received is strictly preserved
      - messages remain available until a consumer processes and deletes it
      - duplicates are not introduced into the queue
      - support message groups that allow multiple ordered message groups within a single queue
      - limited to 300 transactions per second (TPS)
  - messages can be kept in the queue from 1 minute to 14 days
  - the default retention period is 4 days
  - SQS guarantees that your message will be will be processed at least once
  - SQS Visibility Timeout is the amount of time that the message is invisible in the SQS queue after a consumer picks up that message
  - if the consuming code finishes processing before the visibility timeout expires, the message will then be deleted from the queue
  - if the consuming code does not finish within the visibility timeout, the message will become visible again and another consumer will process it
  - the default visibility timeout is 30 seconds
  - increase the visibility timeout if your application will take longer than 30 seconds to process messages
  - the maximum visibility timeout is 12 hours
  - if your application is projected to take more than 12 hours to process a message, SQS is not a good choice
  - SQS Long Polling is a way to retrieve messages from your SQS queues
    - While the regular short polling returns immediately (even if the queue is empty), long polling does not return a response until a message arrives in the queue or the long poll times out
    - long polling can save you money because you are not constantly polling an empty queue
Simple Notification Service
  - 
