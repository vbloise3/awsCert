[
  {
    "id": "16",
    "category": "Cloud Concepts",
    "info": {
      "subcategory": "Storage and Data Management",
      "questionType": "multiple choice 1",
      "question": "You work for a mobile gaming company that has developed a word puzzle game that allows multiple users to challenge each other to complete a crossword puzzle type of game board. This interactive game works on mobile devices and web browsers. You have a world-wide user base that can play against each other no matter where each player is located. You now need to create a leaderboard component of the game architecture where players can look at the daily point leaders for the day, week, or other timeframes. Each time a player accumulates points, the points counter for that player needs to be updated in real-time. This leaderboard data is transient in that it only needs to be stored for a limited duration. Which of the following architectures best suits your data access and retrieval patterns using the simplest, most efficient approach?",
      "answers": [
        "Data sources -> Kinesis Data Streams -> Spark Streaming on EMR -> ElastiCache Redis -> DynamoDB",
        "Data Sources -> Kinesis Data Firehose -> S3 -> Athena",
        "Data sources -> Kinesis Data Streams -> Spark Streaming on EMR -> ElastiCache Memcached -> DynamoDB",
        "Data sources -> Kinesis Data Streams -> Spark Streaming on EMR -> ElastiCache Redis",
        "Data sources -> Kinesis Data Firehose -> Spark Streaming on EMR -> ElastiCache Redis -> S3"
      ],
      "correctAnswer": ["Data sources -> Kinesis Data Streams -> Spark Streaming on EMR -> ElastiCache Redis"]
    }
  },
  {
    "id": "17",
    "category": "Cloud Concepts",
    "info": {
      "subcategory": "Storage and Data Management",
      "questionType": "multiple choice 1",
      "question": "You work for a car manufacturer who has implemented many sensors into their vehicles such as GPS, lane-assist, braking-assist, temperature/humidity, etc. These cars continuously transmit their structured and unstructured sensor data. You need to build a data collection system to capture their data for use in ad-hoc analytics applications to understand the performance of the cars, the locations traveled to and from, the effectiveness of the lane and brake assist features, etc. You also need to filter and transform the sensor data depending on rules based on parameters such as temperature readings. The sensor data needs to be stored indefinitely, however you only wish to pay for the analytics processing when you use it.Which of the following architectures best suits your data lifecycle and usage patterns using the simplest, most efficient approach?",
      "answers": [
        "Sensor data -> Kinesis Data Streams -> IoT Core -> S3 -> Athena",
        "Sensor data -> Kinesis Data Firehose -> IoT Core -> S3 -> Athena",
        "Sensor data -> Kinesis Data Streams -> IoT Core -> Kinesis Data Firehose -> RedShift ->  QuickSight",
        "Sensor data -> IoT Core -> S3 -> Athena",
        "Sensor data -> Kinesis Data Firehose -> S3 -> Athena"
      ],
      "correctAnswer": ["Sensor data -> IoT Core -> S3 -> Athena"]
    }
  },
  {
    "id": "18",
    "category": "Cloud Concepts",
    "info": {
      "subcategory": "Analysis and Visualization",
      "questionType": "multiple choice 1",
      "question": "You work for a public health governmental organization where you are responsible for building out a data warehouse to hold infectious disease information based on the data found at the World Health Organization’s Global Health Observatory data repository. You expect your initial data warehouse to hold less than TBs of data. However, you expect that the data stored in your warehouse will grow rapidly based on the state of world-wide infectious disease progression in the near future.Your organization plans to use the data stored in your data warehouse to visualize disease progression across the various states in your country as infectious diseases progress through their lifecycle. These analyses will be used to make important decisions about citizen interaction and mobility.Which of the following data warehouse configurations best suits your data analysis scenario using the simplest, most cost effective approach?",
      "answers": [
        "Redshift with RA3 nodes",
        "Redshift with DC2 nodes",
        "S3 with SSD volumes",
        "S3 with HDD volumes",
        "Redshift with DS2 nodes"
      ],
      "correctAnswer": ["Redshift with RA3 nodes"]
    }
  },
  {
    "id": "19",
    "category": "Cloud Concepts",
    "info": {
      "subcategory": "Storage and Data Management",
      "questionType": "multiple choice 1",
      "question": "You work for a large city police department as a data scientist. You have been given the task of tracking crime by city district for each criminal committing the given crime. You have created a DynamoDB table to track the crimes across your city’s districts. The table has this configuration: for each crime the table contains a CriminalId (the partition key), CityDistrict, and CrimeDate the crime was reported. Your police department wants to create a dashboard of the crimes reported by district and date. What is the most cost effective way to retrieve the crime data from your DynamoDB table to build your crimes reported by district and date?",
      "answers": [
        "Create a local secondary index with CriminalId as the partition key and CrimeDate as the sort key",
        "Create a global secondary index with CityDistrict as the partition key and CrimeDate as the sort key",
        "Scan the table and use the ProjectionExpression parameter to return the crimes reported by district and date",
        "Scan the secondary index and use the ProjectionExpression parameter to return the crimes reported by district and date"
      ],
      "correctAnswer": ["Create a global secondary index with CityDistrict as the partition key and CrimeDate as the sort key"]
    }
  },
  {
    "id": "20",
    "category": "Cloud Concepts",
    "info": {
      "subcategory": "Storage and Data Management",
      "questionType": "multiple choice 1",
      "question": "You work for a large retail and wholesale business with a significant ecommerce web presence. Your company has just acquired a new ecommerce clothing line and needs to build a data warehouse for this new line of business. The acquired ecommerce business sells clothing to a niche market of men’s casual and business attire. You have chosen to use Amazon Redshift for your data warehouse. The data that you’ll initially load into the warehouse will be relatively small. However, you expect the warehouse data to grow as the niche customer base expands once the parent company makes a significant investment in advertising. What is the most cost effective and best performing Redshift strategy that you should use when you create your initial tables in Redshift?",
      "answers": [
        "Use the KEY distribution strategy",
        "Use the EVEN distribution strategy",
        "Use the ALL distribution strategy",
        "Use the AUTO distribution strategy"
      ],
      "correctAnswer": ["Use the AUTO distribution strategy"]
    }
  },
  {
    "id": "21",
    "category": "Cloud Concepts",
    "info": {
      "subcategory": "Storage and Data Management",
      "questionType": "multiple choice 1",
      "question": "You are a data scientist working for a multinational conglomerate corporation that has many data stores for which you need to provide a common repository. All of your company’s systems need to use this common repository to store and retrieve metadata to work with the data stored in all of the data siolos throughout the organization. You also need to provide the ability to query and transform the data in the organization’s data silos. This common repository will be used for data analytics by your data scientist team to produce dashboards and KPIs for your management team.You are using AWS Glue to build your common repository as depicted in this diagram: data stores (S3, DynamoDB, Redshift, RDS) crawled by Glue crawler, loading Glue data catalog which is used by Athena, Redshift Spectrum, and EMR. As you begin to create this common repository you notice that you aren’t getting the inferred schema for some of your data stores. You have run your crawler against your data stores using your custom classifiers. What might be the problem with your process?",
      "answers": [
        "The username you provided to your JDBC connection to your S3 buckets does not have SELECT permission to retrieve metadata from the S3 bucket data store",
        "The username you provided to your JDBC connection to your Redshift clusters does not have SELECT permission to retrieve metadata from the Redshift data store",
        "You did not use the Glue built-in classifiers in your crawler job",
        "The username you provided to your JDBC connection to your DynamoDB tables does not have SELECT permission to retrieve metadata from the DynamoDB data store"
      ],
      "correctAnswer": ["The username you provided to your JDBC connection to your Redshift clusters does not have SELECT permission to retrieve metadata from the Redshift data store"]
    }
  },
  {
    "id": "22",
    "category": "Cloud Concepts",
    "info": {
      "subcategory": "Storage and Data Management",
      "questionType": "multiple choice 1",
      "question": "You are a data scientist working for a retail chain that stores information about their supply chain partners (partner metadata) and their interaction with these partners (products produced, payments processed, competing partners, etc.). You are tasked with building a data store and associated data lifecycle management system for this partner data. The data will be used for analytics in managing these partners to maximize profitability for your supply chain. You need to manage the data lifecycle according to the various access patterns defined for each type while maintaining storage cost efficiency. The partner metadata is less frequently accessed than the partner interaction data. You need to manage your storage costs so that high frequency accessed data (such as your partner interaction data) is available at very fast response times (sub-second), less frequently accessed data (such as your partner metadata) is available in minutes, and your rarely accessed data (such as historical data on former partners) is available within hours.Which storage lifecycle best fits your usage patterns and business requirements?",
      "answers": [
        "Partner interaction data (sub-second response) stored in Redshift, partner metadata (minutes response) stored in S3 Standard, and former partner data (hours response) in S3 Intelligent-Tiering.",
        "Use a Redshift cluster for all of your data. Create RA3 nodes in your cluster for your partner interaction data (sub-second response), create DC2 nodes for your partner metadata (minutes response), and DS2 nodes for your former partner data (hours response).",
        "Partner interaction data (sub-second response) stored in Redshift, partner metadata (minutes response) stored in S3 Standard, and former partner data (hours response) in S3 Glacier.",
        "Partner interaction data (sub-second response) stored in RDS Aurora, partner metadata (minutes response) stored in S3 Standard, and former partner data (hours response) in S3 Glacier."
      ],
      "correctAnswer": ["Partner interaction data (sub-second response) stored in Redshift, partner metadata (minutes response) stored in S3 Standard, and former partner data (hours response) in S3 Glacier."]
    }
  },
  {
    "id": "23",
    "category": "Cloud Concepts",
    "info": {
      "subcategory": "Processing",
      "questionType": "multiple choice 1",
      "question": "You are a data analyst working for a scientific research and data science company that is building a large scale data lake on EMR to house research data for ongoing research projects. Some of the projects have data processing requirements that need hot data set access, while others require less-hot data set access. For example, analysis for political polling related projects requires hot data set access due to the pressing nature of understanding political analytics and trends in real-time. Infrastructure and materials projects have less-hot data set access requirements since these projects have the option of producing their analysis on a daily basis versus a real-time basis.Additionally, the real-time analytics projects require fast performance, their data is considered timely but temporary. However, the less-hot data projects don’t require real-time analytics, they require persistent data storage. Which data processing solution best fits your usage patterns and business requirements?",
      "answers": [
        "S3 BFS for the hot data sets, S3 Glacier for the less-hot data sets",
        "S3 EMRFS for the hot data sets, HDFS for the less-hot data sets",
        "HDFS for the hot data sets, S3 EMRFS for the less-hot data sets",
        "S3 BFS for the hot data sets, HDFS for the less-hot data sets"
      ],
      "correctAnswer": ["HDFS for the hot data sets, S3 EMRFS for the less-hot data sets"]
    }
  },
  {
    "id": "24",
    "category": "Cloud Concepts",
    "info": {
      "subcategory": "Processing",
      "questionType": "multiple choice 2",
      "question": "You are a data scientist working for a large transportation company that manages its distribution data across all of its distribution lines: trucking, shipping, airfreight, etc. This data is stored in a data warehouse in Redshift. The company ingests all of the distribution data into an EMR cluster before loading the data into their data warehouse in Redshift. The data is loaded from EMR to Redshift on a schedule, once per day.How might you lower the operational costs of running your EMR cluster? (Select TWO)",
      "answers": [
        "EMR Transient Cluster",
        "EMR Long-running Cluster",
        "EMR Core Nodes as spot instances",
        "EMR Task Nodes as spot instances",
        "EMR cluster launched via AWS CLI using defaults"
      ],
      "correctAnswer": ["EMR Transient Cluster",
      "EMR Task Nodes as spot instances"]
    }
  },
  {
    "id": "25",
    "category": "Cloud Concepts",
    "info": {
      "subcategory": "Processing",
      "questionType": "multiple choice 1",
      "question": "You are a data scientist working for an online retail company that wishes to catalog all of their products in a data lake. They also want to load their product data from their data lake into a data warehouse that they can use for business intelligence (BI) dashboards and analytics with QuickSight. How would you automate and operationalize the data processing to get the company’s product data from their data lake to their data warehouse in the most efficient, cost effective manner?",
      "answers": [
        "Product data put to S3 data lake -triggers-> Lambda -runs-> Glue Crawler -> on completion CloudWatch event rule -triggers-> Lambda which runs Glue ETL job that transforms data to JSON -> S3 -triggers-> Lambda which runs COPY command to move data to Redshift",
        "Product data put to S3 data lake -triggers-> Lambda -runs-> Glue Crawler -> on completion CloudWatch event rule -triggers-> Lambda which runs Glue ETL job that transforms data to Parquet -> S3 -triggers-> Lambda which runs COPY command to move data to Redshift",
        "Product data put to S3 data lake -triggers-> Lambda -runs-> Glue Crawler -> on completion CloudWatch event rule -triggers-> Lambda which runs Glue ETL job that transforms data to JSON -> S3 -triggers-> Lambda which runs COPY command to move data to RDS Aurora",
        "Product data put to S3 data lake -triggers-> Lambda -runs-> Glue Crawler -> on completion CloudWatch event rule -triggers-> Lambda which runs Glue ETL job that transforms data to CSV -> S3 -triggers-> Lambda which runs COPY command to move data to Redshift"
      ],
      "correctAnswer": ["Product data put to S3 data lake -triggers-> Lambda -runs-> Glue Crawler -> on completion CloudWatch event rule -triggers-> Lambda which runs Glue ETL job that transforms data to Parquet -> S3 -triggers-> Lambda which runs COPY command to move data to Redshift"]
    }
  },
  {
    "id": "26",
    "category": "Cloud Concepts",
    "info": {
      "subcategory": "Processing",
      "questionType": "multiple choice 1",
      "question": "You work as a data scientist at a large hedge fund. Your firm produces analytics dashboard data for all of its traders. The data that you use is extracted from several trading systems, then transformed by removing canceled trades and classifying trades that remain open as pending. Quite often there are exotic trade types that your analytics application has not processed in past runs. When this happens your data processing solution needs to handle these new types of trades without having to modify the transformation code or the downstream data store.This process is run at the end of each trading day for each trader in the firm. How would you automate and operationalize this data processing flow in the most efficient, cost effective manner?",
      "answers": [
        "A Glue schedule trigger runs at the end of the day which starts two Glue transformation jobs: remove_canceled_trades and classify_open_trades_as_pending. When both of these jobs have completed an event trigger starts a Glue crawler that crawls the transformed data and updates the schema. Upon completion of the crawler schema update, a Glue ETL job runs and uses the COPY command to move the data to Redshift. Analytics dashboards are built using Redshift data.",
        "A Glue schedule trigger runs at the end of the day which starts two Glue transformation jobs: remove_canceled_trades and classify_open_trades_as_pending. When both of these jobs have completed an event trigger starts a Glue crawler that crawls the transformed data and updates the schema. Upon completion of the crawler schema update, a Glue ETL job runs and uses the UNLOAD command to move the data to Redshift. Analytics dashboards are built using Redshift data.",
        "A cron job schedule trigger runs at the end of the day which starts two Glue transformation jobs: remove_canceled_trades and classify_open_trades_as_pending. When both of these jobs have completed a cron job schedule trigger starts a Glue crawler that crawls the transformed data and updates the schema. Upon completion of the crawler schema update, a Glue ETL job runs and uses the COPY command to move the data to Redshift. Analytics dashboards are built using Redshift data.",
        "A Glue schedule trigger runs at the end of the day which starts two Glue transformation jobs: remove_canceled_trades and classify_open_trades_as_pending. When both of these jobs have completed an event trigger starts a Glue crawler that crawls the transformed data and updates the schema. Upon completion of the crawler schema update, a Glue ETL job runs and uses the PUT command to move the data to Redshift. Analytics dashboards are built using Redshift data."
      ],
      "correctAnswer": ["A Glue schedule trigger runs at the end of the day which starts two Glue transformation jobs: remove_canceled_trades and classify_open_trades_as_pending. When both of these jobs have completed an event trigger starts a Glue crawler that crawls the transformed data and updates the schema. Upon completion of the crawler schema update, a Glue ETL job runs and uses the COPY command to move the data to Redshift. Analytics dashboards are built using Redshift data."]
    }
  },
  {
    "id": "27",
    "category": "Cloud Concepts",
    "info": {
      "subcategory": "Processing",
      "questionType": "multiple choice 1",
      "question": "You work as a data scientist at a large global bank. Your bank receives loan information in the form of weekly files from several different loan processing and credit verification agencies. You need to automate and operationalize a data processing solution to take these weekly files, transform them and then finish up by combining them into one file to be ingested into your Redshift data warehouse. The files arrive at different times every week, but the delivering agencies attempt to meet their service level agreement (SLA) of 1:00 AM to 4:00 AM. Unfortunately, the agencies frequently miss their SLAs. You have a tight batch time frame into which you have to squeeze all of this processing.How would you build a data processing system that allows you to gather the agency files and process them for your data warehouse in the most efficient manner and in the shortest time frame?",
      "answers": [
        "Agency files arrive on an S3 bucket. An ETL Lambda function is triggered as each file arrives. The ETL Lambda function transforms the data and writes the transformed file to another S3 bucket. After all of the agency files have been processed by the ETL Lambda function, another Lambda function is triggered to combine the agency file data into one parquet file and write it to another S3 bucket. Then a last Lambda function is triggered to run the COPY command to load the parquet file data into Redshift.",
        "Agency files arrive on an S3 bucket. Use CloudWatch events to schedule a weekly Step Functions state machine. The Step Functions state machine calls a Lambda function to verify that the agency files have arrived. The state machine then starts several Glue ETL jobs in parallel to transform the agency data. Once the agency file transformation jobs have completed the state machine starts another Glue ETL job to combine the transformed agency files and convert the data to a parquet file. The parquet file is written to an S3 bucket. Then the state machine finally runs a last Glue ETL job to run the COPY command to load the parquet file data into Redshift.",
        "Agency files arrive on an S3 bucket. An ETL Lambda function is triggered as each file arrives. The ETL Lambda function transforms the data and writes the transformed file to another S3 bucket. After all of the agency files have been processed by the ETL Lambda function, another Lambda function is triggered to combine the agency file data into one CSV file and write it to another S3 bucket. Then a last Lambda function is triggered to run the UNLOAD command to load the CSV file data into Redshift.",
        "Agency files arrive on an S3 bucket. Use CloudWatch events to schedule a weekly Step Functions state machine. The Step Functions state machine calls a Lambda function to verify that the agency files have arrived. The state machine then starts several Glue ETL jobs in parallel to transform the agency data. Once the agency file transformation jobs have completed the state machine starts another Glue ETL job to combine the transformed agency files and convert the data to an ORC file. The ORC file is written to an S3 bucket. Then the state machine finally runs a last Glue ETL job to run the UNLOAD command to load the ORC file data into Redshift."
      ],
      "correctAnswer": ["Agency files arrive on an S3 bucket. Use CloudWatch events to schedule a weekly Step Functions state machine. The Step Functions state machine calls a Lambda function to verify that the agency files have arrived. The state machine then starts several Glue ETL jobs in parallel to transform the agency data. Once the agency file transformation jobs have completed the state machine starts another Glue ETL job to combine the transformed agency files and convert the data to a parquet file. The parquet file is written to an S3 bucket. Then the state machine finally runs a last Glue ETL job to run the COPY command to load the parquet file data into Redshift."]
    }
  },
  {
    "id": "28",
    "category": "Cloud Concepts",
    "info": {
      "subcategory": "Processing",
      "questionType": "multiple choice 1",
      "question": "You work as a cloud architect for a cloud consultancy practice at a major IT consulting firm. Your latest client has a series of data processing Apache Spark ELT jobs that they want to run in a pipeline on EMR. Thay have asked you which set of data processing tools and techniques will best suit their pipeline needs. The jobs have a specified sequence. Your client wants to manage their costs. Therefore, they want to keep the solution simple, they don’t want to build an application to run these jobs, and they don’t want to incur any additional costs on virtual servers to run their pipeline. Also, they plan on integrating their Apache Spark pipeline with other AWS services in the future.Which orchestration tool set best suits your client’s pipeline requirements?",
      "answers": [
        "Apache Oozie to schedule and run the Spark jobs",
        "Apache Airflow to schedule and run the Spark jobs",
        "AWS Step Functions to schedule and run the Spark jobs",
        "AWS Lambda to schedule and run the Spark jobs",
        "AWS DMS to schedule and run the Spark jobs"
      ],
      "correctAnswer": ["AWS Step Functions to schedule and run the Spark jobs"]
    }
  },
  {
    "id": "29",
    "category": "Cloud Concepts",
    "info": {
      "subcategory": "Processing",
      "questionType": "multiple choice 1",
      "question": "You work as a cloud architect for a gaming company that is building an analytics platform for their gaming data. This analytics platform will ingest game data from current games being played by users of their mobile game platform. The game data needs to be loaded into a data lake where business intelligence (BI) tools will be used to build analytics views of key performance indicators (KPIs). You load your data lake from an EMR cluster where you run Glue ETL jobs to perform the transformation of the incoming game data to the parquet file format. Once transformed, the parquet files are stored in your S3 data lake. From there you can run BI tools, such as Athena, to build your KPIs.You want to handle EMR step through recovery logic. What is the simplest way to build retry logic into your data processing solution?",
      "answers": [
        "CloudTrail event rule sends a text message via a Simple Notification Service (SNS) topic, a support engineer reruns the failed EMR step.",
        "CloudWatch event rule sends a text message via a Simple Notification Service (SNS) topic, a support engineer reruns the failed EMR step.",
        "CloudTrail event rule triggers a Lambda function via a Simple Notification Service (SNS) topic which retries the EMR step.",
        "CloudWatch event rule triggers a Lambda function via a Simple Notification Service (SNS) topic which retries the EMR step.",
        "CloudWatch event rule triggers a retry of the Spark step via a Simple Notification Service (SNS) topic."
      ],
      "correctAnswer": ["CloudWatch event rule triggers a Lambda function via a Simple Notification Service (SNS) topic which retries the EMR step."]
    }
  },
  {
    "id": "30",
    "category": "Cloud Concepts",
    "info": {
      "subcategory": "Security",
      "questionType": "multiple choice 1",
      "question": "You work as a cloud security architect for a financial services company. Your company has an EMR cluster that is integrated with their AWS Lake Formation managed data lake. You use the Lake Formation service to enforce column-level access control driven by policies you have defined. You need to implement a real-time alert and notification system if authenticated users run the TerminateJobFlows, DeleteSecurityConfiguration, or CancelSteps actions within EMR.How would you implement this real-time alert mechanism in the simplest way possible?",
      "answers": [
        "Create a CloudTrail trail and enable continuous delivery of events to an S3 bucket. Use the aws cloudtrail create-trail CLI command to create an SNS topic. When an event occurs a Simple Queue Service (SQS) queue that subscribes to the SNS topic will receive the message. Use a Lambda function triggered by SQS to filter the messages for the TerminateJobFlows, DeleteSecurityConfiguration, or CancelSteps actions. The Lambda function will notify security alert subscribers via another SNS topic.",
        "Create a CloudWatch event and enable continuous delivery of events to an S3 bucket. Use the aws cloudwatch create-event CLI command to create an SNS topic. When an event occurs for the TerminateJobFlows, DeleteSecurityConfiguration, or CancelSteps actions subscribers to the SNS topic will be notified.",
        "Create a Lambda function that subscribes to an SNS topic that you define. The Lambda function will be triggered every time a TerminateJobFlows, DeleteSecurityConfiguration, or CancelSteps action is written to the EMR logs.",
        "Create a CloudTrail trail and enable continuous delivery of events to an S3 bucket. Use the aws cloudtrail create-trail CLI command to create an SNS topic. When an event occurs for the TerminateJobFlows, DeleteSecurityConfiguration, or CancelSteps actions SNS will notify security alert subscribers."
      ],
      "correctAnswer": ["Create a CloudTrail trail and enable continuous delivery of events to an S3 bucket. Use the aws cloudtrail create-trail CLI command to create an SNS topic. When an event occurs a Simple Queue Service (SQS) queue that subscribes to the SNS topic will receive the message. Use a Lambda function triggered by SQS to filter the messages for the TerminateJobFlows, DeleteSecurityConfiguration, or CancelSteps actions. The Lambda function will notify security alert subscribers via another SNS topic."]
    }
  },
  {
    "id": "31",
    "category": "Cloud Concepts",
    "info": {
      "subcategory": "Processing",
      "questionType": "multiple choice 1",
      "question": "You work as a data scientist for a medical data processing company. Your company receives patient data via file feeds into one of your S3 buckets. The data is formatted as a nested JSON document similar to this: JSON with id, category, infor with nested subcategory, questionType, question, answers, correctAnswer. After performing data engineering on some sample files you have noticed occasional inconsistencies in the data types in the JSON. What is the most performant and cost effective way to clean your semi-structured JSON data?",
      "answers": [
        "Run an AWS Batch job that uses the dirtyjson library",
        "Use an EMR job that uses the Spark native DataFrame API",
        "Trigger a Lambda function that uses the json.load and json.loads libraries",
        "Run a Glue job that uses the DynamicFrame extension"
      ],
      "correctAnswer": ["Run a Glue job that uses the DynamicFrame extension"]
    }
  },
  {
    "id": "32",
    "category": "Cloud Concepts",
    "info": {
      "subcategory": "Collection",
      "questionType": "multiple choice 1",
      "question": "You work as a data scientist for a rideshare company. Rideshare request data is collected in one of the company’s S3 buckets (inbound bucket). This data needs to be processed (transformed) very quickly, within seconds of being put onto the S3 bucket. Once transformed, the rideshare request data must be put into another S3 bucket (transformed bucket) where it will be processed to link rideshare drivers with rideshare requesters. You have already written Spark jobs to do the transformation. You need to control costs and minimize data latency for the rideshare request transformation operationalization of your data collection system. Which option best meets your requirements?",
      "answers": [
        "Lambda function triggered when the rideshare data request is put onto the inbound S3 bucket. Lambda sends an SNS topic to an SQS queue. Another Lambda function polls the queue every minute and when it finds a message it launches an EMR cluster and submits a Spark job to process the request.",
        "Lambda function triggered when the rideshare data request is put onto the inbound S3 bucket. The Lambda function passes the request data to a Spark job in Glue.",
        "Lambda function triggered when the rideshare data request is put onto the inbound S3 bucket. The Lambda function launches an EMR cluster and submits the job using the EMR Steps API to process the request.",
        "Build an EMR cluster that runs Apache Livy. Lambda function triggered when the rideshare data request is put onto the inbound S3 bucket. The Lambda function passes the request data to a Spark job on the EMR cluster."
      ],
      "correctAnswer": ["Build an EMR cluster that runs Apache Livy. Lambda function triggered when the rideshare data request is put onto the inbound S3 bucket. The Lambda function passes the request data to a Spark job on the EMR cluster."]
    }
  },
  {
    "id": "33",
    "category": "Cloud Concepts",
    "info": {
      "subcategory": "Analysis and Visualization",
      "questionType": "multiple choice 1",
      "question": "You are a data scientist working on a preventative health research project using the Global Health Observatory data repository. This repository contains the Body Mass Index (BMI) dataset which is based on several thousand observations from around the globe from 1975 to 2016. You need to analyze this dataset using QuickSite. One of the visuals you’ve been asked to create is to show the prevalence of thinness by country across the globe from 1975 to 2016 at 5-year increments. What is the best visual type to use to display this data?",
      "answers": [
        "Geospatial chart",
        "Bubble chart",
        "Heat map",
        "Tree map"
      ],
      "correctAnswer": ["Geospatial chart"]
    }
  },
  {
    "id": "34",
    "category": "Cloud Concepts",
    "info": {
      "subcategory": "Analysis and Visualization",
      "questionType": "multiple choice 1",
      "question": "You are a data scientist working for the Fédération Internationale de Football Association (FIFA). Your management team has asked you to select the appropriate data analysis solution to analyze streaming football data in near real-time. You need to use this data to build interactive results through graphics and interactive charts for the FIFA management team. The football streaming events are based on time series that are unordered and may frequently be duplicated. You also need to transform the football data before you store it.  You’ve been instructed to focus on providing high quality functionality based on fast data access. Which solution best fits your needs?",
      "answers": [
        "Kinesis data firehose -> ORC files -> S3 -> Athena",
        "Kinesis data firehose -> Lambda -> Elasticsearch Cluster -> Kibana",
        "Kinesis data firehose -> RDS -> QuickSight",
        "Kinesis Data Streams -> parquet files -> S3 -> Redshift Spectrum"
      ],
      "correctAnswer": ["Kinesis data firehose -> Lambda -> Elasticsearch Cluster -> Kibana"]
    }
  },
  {
    "id": "35",
    "category": "Cloud Concepts",
    "info": {
      "subcategory": "Security",
      "questionType": "multiple choice 1",
      "question": "You are a data scientist working for a large bank where you are building out an EMR cluster for their customer information data lake. Due to the Personally Identifiable Information (PII) stored in the data lake, you need to lock down all environments (dev, engineering, test, perf, prod) to make sure only the appropriate users and user groups have access to the data lake.To accomplish this goal you have created this IAM policy and attached it to your users and user groups who will be working with your EMR cluster: policy with an Effect of Allow for all elasticmapreduce actions with a Condition of StringEquals ResourceTag/department of dev and eng. How does this policy protect your EMR cluster that contains the company’s customer PII data?",
      "answers": [
        "It prevents users from performing the actions listed in the Action part of the Statement",
        "It allows users to perform only the actions listed in the Action part of the Statement",
        "It only allows users to perform the actions listed in the Action part of the Statement if the EMR cluster they are attempting to access is tagged as department of dev or eng",
        "It only allows users to perform the actions listed in the Action part of the Statement if the EMR cluster they are attempting to access is NOT tagged as department of dev or eng"
      ],
      "correctAnswer": ["It only allows users to perform the actions listed in the Action part of the Statement if the EMR cluster they are attempting to access is tagged as department of dev or eng"]
    }
  },
  {
    "id": "36",
    "category": "Cloud Concepts",
    "info": {
      "subcategory": "Security",
      "questionType": "multiple choice 2",
      "question": "You are a data scientist working for a financial services firm where you are building out an EMR cluster used to house the data lake used for your company’s proprietary machine learning models that predict market movement in global markets. The data in this data lake is considered to be a fundamental part of the company’s knowledge capital so it can only be accessed by users in the Quantitative Equity Group defined within IAM. You need to lock down all environments (dev, engineering, test, perf, prod) to make sure only the users in the Quantitative Equity Group have access to the data lake.To accomplish this goal you have created this IAM policy and attached it to your users in the Quantitative Equity Group IAM group who will be working with your confidential EMR cluster:  policy with an Effect of Allow for all elasticmapreduce actions with a Condition of StringEquals ResourceTag/department of dev, eng, test, perf, prod. You then created this policy and attached it to all users to further lockdown the EMR cluster environments: policy with Effect of Deny of AddTags and RemoveTags actions with a Condition of StringEquals ResourceTag/department of dev, eng, test, perf, prod. What further protection does this policy give you (SELECT TWO)?",
      "answers": [
        "It prevents users from adding or removing tags on an EMR cluster that is tagged with the department of dev, eng, test, perf, or prod. This prevents users from giving themselves access to the EMR cluster by adding a tag to the cluster that their user profile allows",
        "It prevents users from adding or removing tags on an EMR cluster that is tagged with the department of dev, eng, test, perf, or prod. This prevents users from giving anyone access to the EMR cluster by adding tags to the cluster, opening up access to any user",
        "It prevents users from adding or removing tags on an EMR cluster that is tagged with the department of dev, eng, test, perf, or prod. This prevents users from giving themselves access to the EMR cluster by removing the tags on the cluster, opening up access to any user even if they aren’t in the Quantitative Equity Group IAM group",
        "It prevents users from adding or removing tags on an EMR cluster that is tagged with the department of dev, eng, test, perf, or prod. This prevents users from giving themselves access to the EMR cluster by removing a tag on the cluster, opening up access to any user who is allowed to act on resources with that tag",
        "It prevents users from adding or removing tags on an EMR cluster that is tagged with the department of dev, eng, test, perf, or prod. This prevents users from creating a new EMR cluster by cloning the Quantitative Equity Group owned cluster into the new cluster"
      ],
      "correctAnswer": ["It prevents users from adding or removing tags on an EMR cluster that is tagged with the department of dev, eng, test, perf, or prod. This prevents users from giving themselves access to the EMR cluster by adding a tag to the cluster that their user profile allows",
      "It prevents users from adding or removing tags on an EMR cluster that is tagged with the department of dev, eng, test, perf, or prod. This prevents users from giving themselves access to the EMR cluster by removing the tags on the cluster, opening up access to any user even if they aren’t in the Quantitative Equity Group IAM group"]
    }
  },
  {
    "id": "37",
    "category": "Cloud Concepts",
    "info": {
      "subcategory": "Security",
      "questionType": "multiple choice 1",
      "question": "You are a data scientist working for a medical services firm where you are building out an EMR cluster used to house the data lake used for your company’s client healthcare protected health information (PHI) data. The storage of this type of data is highly regulated through the Health Insurance Portability and Accountability Act (HIPAA). Specifically, HIPAA requires that  healthcare companies, like your company, encrypt their client’s PHI data using encryption technology.You have set up your EMR cluster to use the default of using the EMRFS to read and write your client’s PHI data to and from S3. You need to encrypt your client’s PHI data before you send it to S3.Which option is the best encryption technique to use for your EMR cluster configuration?",
      "answers": [
        "SSE-S3",
        "SSE-KMS",
        "CSE-KMS",
        "SSE-C"
      ],
      "correctAnswer": ["CSE-KMS"]
    }
  },
  {
    "id": "38",
    "category": "Cloud Concepts",
    "info": {
      "subcategory": "Security",
      "questionType": "multiple choice 1",
      "question": "You are a data scientist working for a retail clothing manufacturer that has a large online presence through their retail website. The website gathers Personally Identifiable Information (PII), such as credit card numbers, when customers complete their purchases on the website. Therefore, your company must adhere to the Payment Card Industry Data Security Standard (PCI DSS). Your company wishes to store the client data and purchase information data gathered through these transactions in their data warehouse, running on Redshift, where they intend to build Key Performance Indicator (KPI) dashboards using QuickSight. You and your security department know that your data collection system needs to obfuscate the PII (credit card) data, gathered through your data collection system. How should you protect the highly sensitive credit card data in order to meet the PCI DSS requirements while keeping your data collection system as efficient and cost effective as possible?",
      "answers": [
        "AWS Shield Advanced for website traffic",
        "AWS WAF for website traffic",
        "Tokenization PII data",
        "Use GuardDuty for website traffic",
        "KMS encryption in transit and at rest"
      ],
      "correctAnswer": ["Tokenization PII data"]
    }
  },
  {
    "id": "39",
    "category": "Cloud Concepts",
    "info": {
      "subcategory": "Security",
      "questionType": "multiple choice 1",
      "question": "You are a data scientist working for a sports gambling company that produces sports betting data for inclusion in mainstream sports websites. Your company’s data is proprietary and needs to be protected for copyright purposes. You have been tasked with creating a data lake on S3 and also loading a relational database that stores your sports data. Any parameters (such as database connection information) used when building analytics applications used to access the data lake and/or database need to be stored in a secure service that encrypts the parameters. Your management team also has the requirement that parameters like database connection information be rotated automatically. What AWS service should you use to protect the media content and metadata?",
      "answers": [
        "AWS IAM",
        "AWS Systems Manager Parameter Store",
        "KMS encryption",
        "AWS Secrets Manager"
      ],
      "correctAnswer": ["AWS Secrets Manager"]
    }
  },
  {
    "id": "40",
    "category": "Cloud Concepts",
    "info": {
      "subcategory": "Security",
      "questionType": "multiple choice 1",
      "question": "You are a data scientist working for a healthcare company that needs to comply with Health Insurance Portability and Accountability Act (HIPAA) regulations. Your company needs to take all of their patient’s data, including test diagnostic data, wearable sensor data, diagnostic data from all doctor visits, etc. and store it in a data lake. They then want to use Athena and other Business Intelligence (BI) tools to query the patient data to enable their healthcare providers to give optimal service to their patients. In order to apply the appropriate data governance and compliance controls, what AWS service(s) will allow you to provide the appropriate (HIPAA) reports? Also, what AWS service(s) will allow you to monitor changes to your data lake S3 bucket ACLs and bucket policies to scan for public read/write access violations?",
      "answers": [
        "CloudTrail to gather the data for the Business Associate Addendum (BAA) HIPAA compliance report. Use custom rules in AWS Config to track and report on S3 ACL and/or bucket policy changes that violate your security policies.",
        "CloudWatch to gather the data for the Business Associate Addendum (BAA) HIPAA compliance report. Use custom rules in AWS Resource Access Manager to track and report on S3 ACL and/or bucket policy changes that violate your security policies.",
        "AWS Artifact to retrieve the Business Associate Addendum (BAA) HIPAA compliance report. Use custom rules in AWS Config to track and report on S3 ACL and/or bucket policy changes that violate your security policies.",
        "AWS Artifact to retrieve the Business Associate Addendum (BAA) HIPAA compliance report. Use custom rules in AWS Resource Access Manager to track and report on S3 ACL and/or bucket policy changes that violate your security policies."
      ],
      "correctAnswer": ["AWS Artifact to retrieve the Business Associate Addendum (BAA) HIPAA compliance report. Use custom rules in AWS Config to track and report on S3 ACL and/or bucket policy changes that violate your security policies."]
    }
  }
]