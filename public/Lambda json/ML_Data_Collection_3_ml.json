[
  {
    "id": "293",
    "category": "Cloud Concepts",
    "info": {
      "subcategory": "Data Collection",
      "questionType": "multiple choice 1",
      "question": "You have been tasked with setting up crawlers in AWS Glue to crawl different data stores to populate your organizations AWS Glue Data Catalogs. Which of the following input data store is NOT an option when creating a crawler?",
      "answers": [
        "JDBC Connections",
        "DocumentDB",
        "DynamoDB",
        "S3",
        "RDS",
        "RedShift"
      ],
      "correctAnswer": ["DocumentDB"]
    }
  },
  {
    "id": "294",
    "category": "Cloud Concepts",
    "info": {
      "subcategory": "Data Collection",
      "questionType": "multiple choice 1",
      "question": "You are a ML specialist working with data that is stored in a distributed EMR cluster on AWS. Currently, your machine learning applications are compatible with the Apache Hive Metastore tables on EMR. You have been tasked with configuring Hive to use the AWS Glue Data Catalog as its metastore. Before you can do this you need to transfer the Apache Hive metastore tables into an AWS Glue Data Catalog. What are the steps you will need to take to achieve this with the LEAST amount of effort?",
      "answers": [
        "Create DMS endpoints for both the input Apache Hive Metastore and the output data store S3 bucket, run a DMS migration to transfer the data, then create a crawler that creates an AWS Glue Data Catalog.",
        "Set up your Apache Hive application with JDBC driver connections, then create a crawler that crawlers the Apache Hive Metastore using the JDBC connection and creates an AWS Glue Data Catalog.",
        "Run a Hive script on EMR that reads from your Apache Hive Metastore, exports the data to an intermediate format in Amazon S3, and then imports that data into the AWS Glue Data Catalog.",
        "Create a Data Pipeline job that reads from your Apache Hive Metastore, exports the data to an intermediate format in Amazon S3, and then imports that data into the AWS Glue Data Catalog."
      ],
      "correctAnswer": ["Run a Hive script on EMR that reads from your Apache Hive Metastore, exports the data to an intermediate format in Amazon S3, and then imports that data into the AWS Glue Data Catalog."]
    }
  },
  {
    "id": "295",
    "category": "Cloud Concepts",
    "info": {
      "subcategory": "Data Collection",
      "questionType": "multiple choice 1",
      "question": "Your organization has given you several different sets of key-value pair JSON files that need to be used for a machine learning project within AWS. What type of data is this classified as and where is the best place to load this data into?",
      "answers": [
        "Structured data, stored in RDS.",
        "Unstructured data, stored in S3.",
        "Semi-structured data, stored in DynamoDB.",
        "Semi-structured data, stored in S3."
      ],
      "correctAnswer": ["Semi-structured data, stored in S3."]
    }
  },
  {
    "id": "296",
    "category": "Cloud Concepts",
    "info": {
      "subcategory": "Data Collection",
      "questionType": "multiple choice 1",
      "question": "You have been tasked with converting multiple JSON files within an S3 bucket to Apache Parquet format. Which AWS service can you use to achieve this with the LEAST amount of effort?",
      "answers": [
        "Create an AWS Glue Job to convert the S3 objects from JSON to Apache Parquet, then output newly formatted files into S3.",
        "Create a Lambda function that reads all of the objects in the S3 bucket. Loop through each of the objects and convert from JSON to Apache Parquet. Once the conversion is complete, output newly formatted files into S3.",
        "Create a Data Pipeline job that reads from your S3 bucket and sends the data the EMR. Create an Apache Spark job to process the data the Apache Parquet and output newly formatted files into S3.",
        "Create an EMR cluster to run an Apache Spark job to process the data the Apache Parquet and output newly formatted files into S3."
      ],
      "correctAnswer": ["Create an AWS Glue Job to convert the S3 objects from JSON to Apache Parquet, then output newly formatted files into S3."]
    }
  },
  {
    "id": "297",
    "category": "Cloud Concepts",
    "info": {
      "subcategory": "Data Collection",
      "questionType": "multiple choice 1",
      "question": "You are a ML specialist within a large organization who needs to run SQL queries and analytics on thousands of Apache logs files stored in S3. Which set of tools can help you achieve this with the LEAST amount of effort?",
      "answers": [
        "AWS Glue Data Catalog and Athena",
        "Redshift and Redshift Spectrum",
        "Data Pipeline and Athena",
        "Data Pipeline and RDS"
      ],
      "correctAnswer": ["AWS Glue Data Catalog and Athena"]
    }
  },
  {
    "id": "298",
    "category": "Cloud Concepts",
    "info": {
      "subcategory": "Data Collection",
      "questionType": "multiple choice 1",
      "question": "In general within your dataset, what is the minimum number of observations you should have compared to the number of features?",
      "answers": [
        "10,000 times as many observations as features.",
        "10 times as many observations as features.",
        "100 times as many observations as features.",
        "1000 times as many observations as features."
      ],
      "correctAnswer": ["10 times as many observations as features."]
    }
  },
  {
    "id": "299",
    "category": "Cloud Concepts",
    "info": {
      "subcategory": "Data Collection",
      "questionType": "multiple choice 1",
      "question": "Which Amazon service allows you to build a high-quality training labeled dataset for your machine learning models? This includes human workers, vendor companies that you choose, or an internal, private workforce.",
      "answers": [
        "S3",
        "Lambda",
        "SageMaker Ground Truth",
        "Jupyter Notebooks"
      ],
      "correctAnswer": ["SageMaker Ground Truth"]
    }
  },
  {
    "id": "300",
    "category": "Cloud Concepts",
    "info": {
      "subcategory": "Data Collection",
      "questionType": "multiple choice 1",
      "question": "An organization needs to store a mass amount of data in AWS. The data has a key-value access pattern, developers need to run complex SQL queries and transactions, and the data has a fixed schema. Which type of data store meets all of their needs?",
      "answers": [
        "Athena",
        "DynamoDB",
        "RDS",
        "S3"
      ],
      "correctAnswer": ["RDS"]
    }
  },
  {
    "id": "301",
    "category": "Cloud Concepts",
    "info": {
      "subcategory": "Data Collection",
      "questionType": "multiple choice 1",
      "question": "You are trying to set up a crawler within AWS Glue that crawls your input data in S3. For some reason after the crawler finishes executing, it cannot determine the schema from your data and no tables are created within your AWS Glue Data Catalog. What is the reason for these results?",
      "answers": [
        "AWS Glue built-in classifiers could not find the input data format. You need to create a custom classifier.",
        "The bucket path for the input data store in S3 is specified incorrectly.",
        "The checkbox for Do not create tables was checked when setting up the crawler in AWS Glue.",
        "The crawler does not have correct IAM permissions to access the input data in the S3 bucket."
      ],
      "correctAnswer": ["AWS Glue built-in classifiers could not find the input data format. You need to create a custom classifier."]
    }
  },
  {
    "id": "302",
    "category": "Cloud Concepts",
    "info": {
      "subcategory": "Data Collection",
      "questionType": "multiple choice 1",
      "question": "When you train your model in SageMaker, where does your training dataset come from?",
      "answers": [
        "S3",
        "RedShift",
        "DynamoDb",
        "RDS"
      ],
      "correctAnswer": ["S3"]
    }
  },
  {
    "id": "303",
    "category": "Cloud Concepts",
    "info": {
      "subcategory": "Data Collection",
      "questionType": "multiple choice 1",
      "question": "You are a ML specialist within a large organization who needs to run SQL queries and analytics on thousands of Apache logs files stored in S3. Your organization already uses Redshift as their data warehousing solution. Which tool can help you achieve this with the LEAST amount of effort?",
      "answers": [
        "S3 Analytics",
        "Redshift Spectrum",
        "Athena",
        "Apache Hive"
      ],
      "correctAnswer": ["Redshift Spectrum"]
    }
  },
  {
    "id": "304",
    "category": "Cloud Concepts",
    "info": {
      "subcategory": "Data Collection",
      "questionType": "multiple choice 1",
      "question": "You are a ML specialist within a large organization who helps job seekers find both technical and non-technical jobs. You have collected data from a data warehouse from an engineering company to determine which skills qualify job seekers for different positions. After reviewing the data you realise the data is biased. Why?",
      "answers": [
        "The data collected needs to be from the general population of job seekers, not just from a technical engineering company.",
        "The data collected is only a few hundred observations making it bias to a small subset of job types.",
        "The data collected has missing values for different skills for job seekers.",
        "The data collected only has a few attributes. Attributes like skills and job title are not included in the data."
      ],
      "correctAnswer": ["The data collected needs to be from the general population of job seekers, not just from a technical engineering company."]
    }
  },
  {
    "id": "305",
    "category": "Cloud Concepts",
    "info": {
      "subcategory": "Data Collection",
      "questionType": "multiple choice 1",
      "question": "You have been tasked with collecting thousands of PDFs for building a large corpus dataset. The data within this dataset would be considered what type of data?",
      "answers": [
        "Structured",
        "Relational",
        "Unstructured",
        "Semi-structured"
      ],
      "correctAnswer": ["Unstructured"]
    }
  },
  {
    "id": "306",
    "category": "Cloud Concepts",
    "info": {
      "subcategory": "Data Collection",
      "questionType": "multiple choice 1",
      "question": "You are a ML specialist who is setting up an ML pipeline. The amount of data you have is massive and needs to be set up and managed on a distributed system to efficiently run processing and analytics on. You also plan to use tools like Apache Spark to process your data to get it ready for your ML pipeline. Which setup and services can most easily help you achieve this?",
      "answers": [
        "Redshift out-performs Apache Spark and should be used instead.",
        "Elastic Map Reduce (EMR) with Apache Spark installed.",
        "Self-managed cluster of EC2 instances with Apache Spark installed.",
        "Multi AZ RDS Read Replicas with Apache Spark installed."
      ],
      "correctAnswer": ["Elastic Map Reduce (EMR) with Apache Spark installed."]
    }
  },
  {
    "id": "306",
    "category": "Cloud Concepts",
    "info": {
      "subcategory": "Streaming Data",
      "questionType": "multiple choice 1",
      "question": "Which service in the Kinesis family allows you to securely stream video from connected devices to AWS for analytics, machine learning (ML), and other processing?",
      "answers": [
        "Kinesis Video Streams",
        "Kinesis Streams",
        "Kinesis Firehose",
        "Kinesis Data Analytics"
      ],
      "correctAnswer": ["Kinesis Video Streams"]
    }
  },
  {
    "id": "306",
    "category": "Cloud Concepts",
    "info": {
      "subcategory": "Streaming Data",
      "questionType": "multiple choice 1",
      "question": "Which service built by AWS makes it easy to set up a retry mechanism, aggregate records to improve throughput, and automatically submits CloudWatch metrics?",
      "answers": [
        "Kinesis Client Library (KCL)",
        "Kinesis Producer Library (KPL)",
        "Kinesis Consumer Library",
        "Kinesis API (AWS SDK)"
      ],
      "correctAnswer": ["Kinesis Producer Library (KPL)"]
    }
  },
  {
    "id": "307",
    "category": "Cloud Concepts",
    "info": {
      "subcategory": "Streaming Data",
      "questionType": "multiple choice 1",
      "question": "Your organization has a standalone Javascript (Node.js) application that streams data into AWS using Kinesis Data Streams. You notice that they are using the Kinesis API (AWS SDK) over the Kinesis Producer Library (KPL). What might be the reasoning behind this?",
      "answers": [
        "The Kinesis Producer Library cannot be integrated with a Javascript application because of its asynchronous architecture.",
        "The Kinesis API (AWS SDK) runs faster in Javascript applications over the Kinesis Producer Library.",
        "The Kinesis Producer Library must be installed as a Java application to use with Kinesis Data Streams.",
        "The Kinesis API (AWS SDK) provides greater functionality over the Kinesis Producer Library."
      ],
      "correctAnswer": ["The Kinesis Producer Library must be installed as a Java application to use with Kinesis Data Streams."]
    }
  },
  {
    "id": "308",
    "category": "Cloud Concepts",
    "info": {
      "subcategory": "Streaming Data",
      "questionType": "multiple choice 1",
      "question": "You are collecting clickstream data from an e-commerce website using Kinesis Data Firehose. You are using the PutRecord API from the AWS SDK to send the data to the stream. What are the required parameters when sending data to Kinesis Data Firehose using the API PutRecord call?",
      "answers": [
        "DataStreamName, PartitionKey, and Record (containing the data)",
        "Data, PartitionKey, StreamName, ShardId",
        "Data, PartitionKey, StreamName",
        "DeliveryStreamName and Record (containing the data)"
      ],
      "correctAnswer": ["DeliveryStreamName and Record (containing the data)"]
    }
  },
  {
    "id": "309",
    "category": "Cloud Concepts",
    "info": {
      "subcategory": "Streaming Data",
      "questionType": "multiple choice 3",
      "question": "You work for a farming company that has dozens of tractors with build-in IoT devices. These devices stream data into AWS using Kinesis Data Streams. The features associated with the data is tractor Id, latitude, longitude, inside temp, outside temp, and fuel level. As an ML specialist you need to transform the data and store it in a data store. Which combination of services can you use to achieve this? (Choose 3)",
      "answers": [
        "Use Kinesis Data Streams to immediately write the data into S3. Next, set up a Lambda function that fires any time an object is PUT onto S3. Transform the data from the Lambda function, then write the transformed data into S3.",
        "Immediately send the data to Lambda from Kinesis Data Streams. Transform the data in Lambda and write the transformed data into S3.",
        "Use Kinesis Data Analytics to run real-time SQL queries to transform the data and immediately write the transformed data into S3.",
        "Set up Kinesis Firehose to ingest data from Kinesis Data Streams, then send data to Lambda. Transform the data in Lambda and write the transformed data into S3.",
        "Set up Kinesis Data Analytics to ingest the data from Kinesis Data Stream, then run real-time SQL queries on the data to transform it. After the data is transformed, ingest the data with Kinesis Data Firehose and write the data into S3."
      ],
      "correctAnswer": ["Immediately send the data to Lambda from Kinesis Data Streams. Transform the data in Lambda and write the transformed data into S3.",
        "Set up Kinesis Firehose to ingest data from Kinesis Data Streams, then send data to Lambda. Transform the data in Lambda and write the transformed data into S3.",
        "Set up Kinesis Data Analytics to ingest the data from Kinesis Data Stream, then run real-time SQL queries on the data to transform it. After the data is transformed, ingest the data with Kinesis Data Firehose and write the data into S3."]
    }
  },
  {
    "id": "310",
    "category": "Cloud Concepts",
    "info": {
      "subcategory": "Streaming Data",
      "questionType": "multiple choice 1",
      "question": "Your organization needs to find a way to capture streaming data from certain events customers are performing. These events are a crucial part of the organizations business development and cannot afford to be lost. You have already set up a Kinesis Data Stream and a consumer EC2 instance to process and deliver the data into S3. You have noticed that the last few days of events are not showing up in S3 and your EC2 instance has been shutdown. What combination of steps can you take to ensure this does not happen again?",
      "answers": [
        "Set up CloudWatch monitoring for your EC2 instance as well as AutoScaling if your consumer EC2 instance is shutdown. Next, set up multiple Kinesis Data Streams to process the data on the EC2 instance.",
        "Set up CloudWatch monitoring for your EC2 instance as well as AutoScaling if your consumer EC2 instance is shutdown. Next, set up a Lambda function to poll the Kinesis Data Stream for failed delivered records and then send those requests back into the consumer EC2 instance.",
        "Set up CloudWatch monitoring for your EC2 instance as well as AutoScaling if your consumer EC2 instance is shutdown. Next, send the data to Kinesis Data Firehose before writing the data into S3. Since Kinesis Data Firehose has retry mechanism built-in, the changes of data being lost is extremely unlikely.",
        "Set up CloudWatch monitoring for your EC2 instance as well as AutoScaling if your consumer EC2 instance is shutdown. Next, ensure that the maximum amount of hours are selected (168 hours) for data retention when creating your Kinesis Data Stream. Finally, write logic on the consumer EC2 instance that handles unprocessed data in the Kinesis Data Stream and failed writes to S3."
      ],
      "correctAnswer": ["Set up CloudWatch monitoring for your EC2 instance as well as AutoScaling if your consumer EC2 instance is shutdown. Next, ensure that the maximum amount of hours are selected (168 hours) for data retention when creating your Kinesis Data Stream. Finally, write logic on the consumer EC2 instance that handles unprocessed data in the Kinesis Data Stream and failed writes to S3."]
    }
  },
  {
    "id": "311",
    "category": "Cloud Concepts",
    "info": {
      "subcategory": "Streaming Data",
      "questionType": "multiple choice 1",
      "question": "Which service in the Kinesis family allows you to easily load streaming data into data stores and analytics tools?",
      "answers": [
        "Kinesis Streams",
        "Kinesis Firehose",
        "Kinesis Video Streams",
        "Kinesis Data Analytics"
      ],
      "correctAnswer": ["Kinesis Firehose"]
    }
  },
  {
    "id": "312",
    "category": "Cloud Concepts",
    "info": {
      "subcategory": "Streaming Data",
      "questionType": "multiple choice 1",
      "question": "You have been tasked with capturing data from an online gaming platform to run analytics on and process through a machine learning pipeline. The data that you are ingesting is players controller inputs every 1 second (up to 10 players in a game) that is in JSON format. The data needs to be ingested through Kinesis Data Streams and the JSON data blob is 100 KB in size. What is the minimum number of shards you can use to successfully ingest this data?",
      "answers": [
        "Greater than 500 shards, so you will need to request more shards from AWS",
        "10 shards",
        "1 shard",
        "100 shards"
      ],
      "correctAnswer": ["1 shard"]
    }
  },
  {
    "id": "313",
    "category": "Cloud Concepts",
    "info": {
      "subcategory": "Streaming Data",
      "questionType": "multiple choice 1",
      "question": "True or False. If you have mission critical data that must be processed with as minimal delay as possible, you should use the Kinesis API (AWS SDK) over the Kinesis Producer Library.",
      "answers": [
        "true",
        "false"
      ],
      "correctAnswer": ["true"]
    }
  },
  {
    "id": "314",
    "category": "Cloud Concepts",
    "info": {
      "subcategory": "Streaming Data",
      "questionType": "multiple choice 1",
      "question": "Which service in the Kinesis family allows you to build custom applications that process or analyze streaming data for specialized needs?",
      "answers": [
        "Kinesis Video Streams",
        "Kinesis Firehose",
        "Kinesis Data Analytics",
        "Kinesis Streams"
      ],
      "correctAnswer": ["Kinesis Streams"]
    }
  },
  {
    "id": "315",
    "category": "Cloud Concepts",
    "info": {
      "subcategory": "Streaming Data",
      "questionType": "multiple choice 1",
      "question": "You are collecting clickstream data from an e-commerce website to make near-real time product suggestions for users actively using the site. Which combination of tools can be used to achieve the quickest recommendations and meets all of the requirements?",
      "answers": [
        "Use the Kinesis Data Analytics to ingest the clickstream data directly and run real time SQL queries to gain actionable insights and trigger real-time recommendations with AWS Lambda functions based on conditions.",
        "Use Kinesis Data Firehose to ingest click stream data, then use Kinesis Data Analytics to run real time SQL queries to gain actionable insights and trigger real-time recommendations with AWS Lambda functions based on conditions, then use Lambda to load these results into S3.",
        "Use Kinesis Data Streams to ingest clickstream data, then use Lambda to process that data and write it to S3. Once the data is on S3, use Athena to query based on conditions that data and make real time recommendations to users.",
        "Use Kinesis Data Streams to ingest clickstream data, then use Kinesis Data Analytics to run real time SQL queries to gain actionable insights and trigger real-time recommendations with AWS Lambda functions based on conditions."
      ],
      "correctAnswer": ["Use Kinesis Data Streams to ingest clickstream data, then use Kinesis Data Analytics to run real time SQL queries to gain actionable insights and trigger real-time recommendations with AWS Lambda functions based on conditions."]
    }
  },
  {
    "id": "316",
    "category": "Cloud Concepts",
    "info": {
      "subcategory": "Streaming Data",
      "questionType": "multiple choice 1",
      "question": "You are an ML specialist needing to collect data from Twitter tweets. You only want to collect tweets that include the name of your company and store those tweets off into a data store in AWS. What set of tools can you use to stream, transform, and load the data into AWS with the LEAST amount of effort?",
      "answers": [
        "Setup a Kinesis Data Stream for data ingestion, setup EC2 instances as data consumers to poll and transform the data from the stream. Once the data is transformed, make an API call to write the data to DynamoDB.",
        "Setup Kinesis Data Streams for data ingestion. Next, setup Kinesis Data Firehouse to load that data into RedShift. Next, setup a Lambda function to query data using RedShift spectrum and store the results onto DynamoDB.",
        "Create a Kinesis Data Stream to ingest the data. Next, setup a Kinesis Data Firehose and use Lambda to transform the data from the Kinesis Data Stream, then use Lambda to write the data to DynamoDB. Finally, use S3 as the data destination for Kinesis Data Firehose.",
        "Setup a Kinesis Data Firehose for data ingestion and immediately write that data to S3. Next, setup a Lambda function to trigger when data lands in S3 to transform it and finally write it to DynamoDB."
      ],
      "correctAnswer": ["Setup a Kinesis Data Firehose for data ingestion and immediately write that data to S3. Next, setup a Lambda function to trigger when data lands in S3 to transform it and finally write it to DynamoDB."]
    }
  },
  {
    "id": "317",
    "category": "Cloud Concepts",
    "info": {
      "subcategory": "Streaming Data",
      "questionType": "multiple choice 1",
      "question": "Which service in the Kinesis family allows you to analyze streaming data, gain actionable insights, and respond to your business and customer needs in real time?",
      "answers": [
        "Kinesis Video Streams",
        "Kinesis Data Analytics",
        "Kinesis Streams",
        "Kinesis Firehose"
      ],
      "correctAnswer": ["Kinesis Data Analytics"]
    }
  },
  {
    "id": "318",
    "category": "Cloud Concepts",
    "info": {
      "subcategory": "Streaming Data",
      "questionType": "multiple choice 1",
      "question": "A local university wants to track cars in a parking lot to determine which students are parking in the lot. The university is wanting to ingest videos of the cars parking in near-real time, use machine learning to identify license plates, and store that data in an AWS data store. Which solution meets these requirements with the LEAST amount of development effort?",
      "answers": [
        "Use Amazon Kinesis Data Streams to ingest videos in near-real time, call Amazon Rekognition to identify license plate information, and then store results in DynamoDB.",
        "Use Amazon Kinesis Data Streams to ingest the video in near-real time, use the Kinesis Data Streams consumer integrated with Amazon Rekognition Video to process the license plate information, and then store results in DynamoDB.",
        "Use Amazon Kinesis Firehose to ingest the video in near-real time and outputs results onto S3. Set up a Lambda function that triggers when a new video is PUT onto S3 to send results to Amazon Rekognition to identify license plate information, and then store results in DynamoDB.",
        "Use Amazon Kinesis Video Streams to ingest the videos in near-real time, use the Kinesis Video Streams integration with Amazon Rekognition Video to identify the license plate information, and then store the results in DynamoDB."
      ],
      "correctAnswer": ["Use Amazon Kinesis Video Streams to ingest the videos in near-real time, use the Kinesis Video Streams integration with Amazon Rekognition Video to identify the license plate information, and then store the results in DynamoDB."]
    }
  },
  {
    "id": "319",
    "category": "Cloud Concepts",
    "info": {
      "subcategory": "Streaming Data",
      "questionType": "multiple choice 1",
      "question": "You have been tasked with capturing two different types of streaming events. The first event type includes mission critical data that needs to immediately be processed before operations can continue. The second event type includes data of less importance, but processing can continue without processing. What is the most appropriate solution to record these different types of events?",
      "answers": [
        "Capture both event types using the Kinesis Producer Library (KPL).",
        "Capture the mission critical events with the PutRecords API call and the second event type with the Kinesis Producer Library (KPL).",
        "Capture both events with the PutRecords API call.",
        "Capture the mission critical events with the Kinesis Producer Library (KPL) and the second event type with the Putrecords API call."
      ],
      "correctAnswer": ["Capture the mission critical events with the PutRecords API call and the second event type with the Kinesis Producer Library (KPL)."]
    }
  },
  {
    "id": "320",
    "category": "Cloud Concepts",
    "info": {
      "subcategory": "Streaming Data",
      "questionType": "multiple choice 3",
      "question": "What are your options for storing data into S3? (Choose 3)",
      "answers": [
        "UPLOAD command",
        "The AWS console",
        "PutRecords API call",
        "AWS CLI",
        "UNLOAD command",
        "AWS SDK"
      ],
      "correctAnswer": ["The AWS console",
        "AWS CLI",
        "AWS SDK"]
    }
  },
  {
    "id": "321",
    "category": "Cloud Concepts",
    "info": {
      "subcategory": "Data Engineering",
      "questionType": "multiple choice 1",
      "question": "You are na ML specialist preparing some labeled data to help determine whether a given leaf originates from a poisonous plant. The target attribute is poisonous and is classified as 0 or 1. The data that you have been analyzing has the following features: leaf height (cm), leaf length (cm), number of cells (trillions), poisonous (binary). After initial analysis you do not suspect any outliers in any of the attributes. After using the data given to train your model, you are getting extremely skewed results. What technique can you apply to possibly help solve this issue?",
      "answers": [
        "Drop the number of cells attribute.",
        "Apply one-hot encoding to each of the attributes, except for the poisonous attribute (since it is already encoded).",
        "PutRecords API call",
        "Standardize the number of cells attribute.",
        "Normalize the number of cells attribute."
      ],
      "correctAnswer": ["Normalize the number of cells attribute."]
    }
  },
  {
    "id": "322",
    "category": "Cloud Concepts",
    "info": {
      "subcategory": "Data Engineering",
      "questionType": "multiple choice 1",
      "question": "We are analyzing the following text { Hello cloud gurus! Keep being awesome! }. We apply lowercase transformation, remove punctuation and n-gram with a sliding window of 3. What are the unique trigrams produced? What are the dimensions of the tf–idf vector/matrix?",
      "answers": [
        "[hello cloud gurus!, cloud gurus keep, gurus keep being, keep being awesome.] and (1, 4)",
        "[hello cloud gurus, cloud gurus keep, gurus keep being, keep being awesome] and (2, 4)",
        "[cloud gurus keep, gurus keep being, hello cloud gurus, keep being awesome] and (1, 4)",
        "[hello cloud gurus, cloud gurus keep, keep being awesome] and (1, 3)"
      ],
      "correctAnswer": ["[cloud gurus keep, gurus keep being, hello cloud gurus, keep being awesome] and (1, 4)"]
    }
  },
  {
    "id": "323",
    "category": "Cloud Concepts",
    "info": {
      "subcategory": "Data Engineering",
      "questionType": "multiple choice 1",
      "question": "You are an ML specialist who is working within SageMaker analyzing a dataset in a Jupyter notebook. On your local machine you have several open-source Python libraries that you have downloaded from the internet using a typical package manager. You want to download and use these same libraries on your dataset in SageMaker within your Jupyter notebook. What options allow you to use these libraries?",
      "answers": [
        "Upload the library in .zip format into S3 and use the Jupyter notebook in SageMaker to reference S3 bucket with Python libraries.",
        "SageMaker offers a wide variety of built-in libraries. If the library you need is not included, contact AWS support with details on libraries needed for distribution.",
        "SSH into the Jupyter notebook instance and install needed libraries. This is typically done using conda install or pip install.",
        "Use the integrated terminals in SageMaker to install libraries. This is typically done using conda install or pip install."
      ],
      "correctAnswer": ["Use the integrated terminals in SageMaker to install libraries. This is typically done using conda install or pip install."]
    }
  },
  {
    "id": "324",
    "category": "Cloud Concepts",
    "info": {
      "subcategory": "Data Engineering",
      "questionType": "multiple choice 1",
      "question": "You work for an organization that wants to manage all of the data stores in S3. The organization wants to automate the transformation jobs on the S3 data and maintain a data catalog of the metadata concerning the datasets. The solution that you choose should require the least amount of setup and maintenance. Which solution will allow you to achieve this and achieve its goals?",
      "answers": [
        "Create a cluster in EMR that uses Apache Hive. Then, create a simple Hive script that runs transformation jobs on a schedule.",
        "Create an AWS Data Pipeline that transforms the data. Then, create an Apache Hive metastore and a script that runs transformation jobs on a schedule.",
        "Create an AWS Glue crawler to populate the AWS Glue Data Catalog. Then, create an AWS Glue job, and set up a schedule for data transformation jobs.",
        "Create a cluster in EMR that uses Apache Spark. Then, create an Apache Hive metastore and a script that runs transformation jobs on a schedule."
      ],
      "correctAnswer": ["Create an AWS Glue crawler to populate the AWS Glue Data Catalog. Then, create an AWS Glue job, and set up a schedule for data transformation jobs."]
    }
  },
  {
    "id": "325",
    "category": "Cloud Concepts",
    "info": {
      "subcategory": "Data Engineering",
      "questionType": "multiple choice 1",
      "question": "A term frequency–inverse document frequency (tf–idf) matrix using both unigrams and bigrams is built from a text corpus consisting of the following two sentences: { Hello world } and { Hello how are you }. What are the dimensions of the tf–idf vector/matrix?",
      "answers": [
        "(2,5)",
        "(2,10)",
        "(2,6)",
        "(2,9)",
        "(5,9)"
      ],
      "correctAnswer": ["(2,9)"]
    }
  },
  {
    "id": "326",
    "category": "Cloud Concepts",
    "info": {
      "subcategory": "Data Engineering",
      "questionType": "multiple choice 2",
      "question": "What are the programming languages offered in AWS Glue for Spark job types? (Choose 2)",
      "answers": [
        "Java",
        "Scala",
        "Python",
        "R",
        "C#"
      ],
      "correctAnswer": ["Scala",
        "Python"]
    }
  },
  {
    "id": "327",
    "category": "Cloud Concepts",
    "info": {
      "subcategory": "Data Engineering",
      "questionType": "multiple choice 1",
      "question": "You are an ML specialist preparing a dataset for a supervised learning problem. You are using the Amazon SageMaker Linear Learner algorithm. You notice the target label attributes are highly imbalanced and multiple feature columns contain missing values. The proportion of missing values across the entire dataset is less than 5%. What should you do to minimize bias due to missing values?",
      "answers": [
        "Drop all of the rows that contain missing values because they represent less than 5% of the data.",
        "For each feature that is missing, use a supervised learning to approximate the values based on other features.",
        "First normalize the non-missing values then replace the missing values with the normalized values.",
        "Replace the missing values with mean or median values from the other values of the same feature."
      ],
      "correctAnswer": ["For each feature that is missing, use a supervised learning to approximate the values based on other features."]
    }
  },
  {
    "id": "328",
    "category": "Cloud Concepts",
    "info": {
      "subcategory": "Data Engineering",
      "questionType": "multiple choice 1",
      "question": "You are working for an organization that takes different metrics about its customers and classifies them with one of the following statuses: bronze, silver, and gold. Depending on their status they get more/less discounts and are placed as a higher/lower priority for customer support. The algorithm you have chosen expects all numerical inputs. What can be done to handle these status values?",
      "answers": [
        "Use one-hot encoding techniques to map values for each status.",
        "Experiment with mapping different values for each status and see which works best.",
        "Use one-hot encoding techniques to map values for each status dropping the original status feature.",
        "Apply random numbers to each status value and apply gradient descent until the values converge to expect results."
      ],
      "correctAnswer": ["Experiment with mapping different values for each status and see which works best."]
    }
  },
  {
    "id": "329",
    "category": "Cloud Concepts",
    "info": {
      "subcategory": "Data Engineering",
      "questionType": "multiple choice 1",
      "question": "You are an ML specialist that has been tasked with setting up a transformation job for 900 TB of data. You have set up several ETL jobs written in Pyspark on AWS Glue to transform your data, but the ETL jobs are taking a very long time to process and it is extremely expensive. What are your other options for processing the data?",
      "answers": [
        "Create Kinesis Data Stream to stream the data to multiple EC2 instances each performing partition workloads and ETL jobs. Tweak cluster size, instance types, and data partitioning until performance and cost satisfaction is met.",
        "Change job type to Python shell and use built-in libraries to perform the ETL jobs. The built-in libraries perform better than Spark jobs and are a fraction of the cost.",
        "Create an EMR cluster with Spark, Hive, and Flink to perform the ETL jobs. Tweak cluster size, instance types, and data partitioning until performance and cost satisfaction is met.",
        "Offload the data to Redshift and perform transformation from Redshift rather than S3. Setup AWS Glue jobs to use Redshift as input data store, then run ETL jobs on batches of Redshift data. Adjust the batch size until performance and cost satisfaction is met."
      ],
      "correctAnswer": ["Create an EMR cluster with Spark, Hive, and Flink to perform the ETL jobs. Tweak cluster size, instance types, and data partitioning until performance and cost satisfaction is met."]
    }
  },
  {
    "id": "330",
    "category": "Cloud Concepts",
    "info": {
      "subcategory": "Data Engineering",
      "questionType": "multiple choice 1",
      "question": "You are an ML specialist who has a Python script using libraries like Boto3, Pandas, NumPy, and sklearn to help transform data that is in S3. On your local machine the data transformation is working as expected. You need to find a way to schedule this job to run periodically and store the transformed data back into S3. What is the best option to use to achieve this?",
      "answers": [
        "Create an AWS Glue job that uses Spark as the job type to create Scala code to transform and store data in S3. Then set up this job to run on some schedule.",
        "Create an AWS Glue job that uses Python shell as the job type and executes the code written to transform and store data in S3. Then set up this job to run on some schedule.",
        "Create an EMR cluster that runs Apache Spark code to transform and store data in S3. Then set up this job to run on some schedule.",
        "Create an AWS Glue job that uses Spark as the job type to create Pyspark code to transform and store data in S3. Then set up this job to run on some schedule."
      ],
      "correctAnswer": ["Create an AWS Glue job that uses Python shell as the job type and executes the code written to transform and store data in S3. Then set up this job to run on some schedule."]
    }
  },
  {
    "id": "331",
    "category": "Cloud Concepts",
    "info": {
      "subcategory": "Data Engineering",
      "questionType": "multiple choice 3",
      "question": "Choose the scenarios in which one-hot encoding techniques are NOT a good idea. (Choose 3)",
      "answers": [
        "When our algorithm expects numeric input and we have few nominal categorical values.",
        "When our algorithm accepts numeric input and we have continuous values.",
        "When our algorithm expects numeric input and we have ordinal categorical values.",
        "When our algorithm expects numeric input and we have thousands of nominal categorical values.",
        "When our values cannot be ordered in any meaningful way, there are only a few to choose from, and our algorithm expects numeric input."
      ],
      "correctAnswer": ["When our algorithm expects numeric input and we have thousands of nominal categorical values.",
        "When our algorithm accepts numeric input and we have continuous values.",
        "When our algorithm expects numeric input and we have ordinal categorical values."]
    }
  },
  {
    "id": "332",
    "category": "Cloud Concepts",
    "info": {
      "subcategory": "Data Engineering",
      "questionType": "multiple choice 1",
      "question": "You are an ML specialist who has 780 GB of files in a data lake-hosted S3. The metadata about these files is stored in the S3 bucket as well. You need to search through the data lake to get a better understanding of what the data consists of. You will most likely do multiple searches depending on results found throughout your research. Which solution meets the requirements with the LEAST amount of effort?",
      "answers": [
        "Use Amazon Athena to analyze and query your S3 data.",
        "Create an EMR cluster with Apache Hive to analyze and query your data.",
        "First, enable S3 analytics then use the metastore files to analyze your data.",
        "Create a Redshift cluster that uses S3 as the input data course, and use Redshift Spectrum to analyze and query your S3 data."
      ],
      "correctAnswer": ["Use Amazon Athena to analyze and query your S3 data."]
    }
  },
  {
    "id": "333",
    "category": "Cloud Concepts",
    "info": {
      "subcategory": "Data Engineering",
      "questionType": "multiple choice 1",
      "question": "You are an ML specialist that has been tasked with setting up an ETL pipeline for your organization. The team already has a EMR cluster that will be used for ETL tasks and needs to be directly integrated with Amazon SageMaker without writing any specific code to connect EMR to SageMaker. Which framework allows you to achieve this?",
      "answers": [
        "Apache Flink",
        "Apache Mahout",
        "Apache Pig",
        "Apache Hive",
        "Apache Spark"
      ],
      "correctAnswer": ["Apache Spark"]
    }
  },
  {
    "id": "334",
    "category": "Cloud Concepts",
    "info": {
      "subcategory": "Data Engineering",
      "questionType": "multiple choice 1",
      "question": "An ML specialist is working for a bank and trying to determine if credit card transactions are fraudulent or non-fraudulent. The features of the data collected include things like customer name, customer type, transaction amount, length of time as a customer, and transaction type. The transaction type is classified as normal and abnormal. What data preparation action should the ML specialist take?",
      "answers": [
        "Drop the customer name and and perform label encoding on the transaction type before training the model.",
        "Drop the length of time as a customer and perform label encoding on the transaction type before training the model.",
        "Drop both the customer type and the transaction type before training the model.",
        "Drop the transaction type and perform label encoding on the customer type before training the model."
      ],
      "correctAnswer": ["Drop the customer name and and perform label encoding on the transaction type before training the model."]
    }
  },
  {
    "id": "335",
    "category": "Cloud Concepts",
    "info": {
      "subcategory": "Exploratory Data Analysis",
      "questionType": "multiple choice 1",
      "question": "You are an ML specialist building a regression model to predict the amount of rainfall for the upcoming year. The data you have contains 18,000 observations collected over the last 50 years. Each observation contains the date, amount of rainfall (in cm), humidity, city, and state. You plot the values in a scatter plot for a given day and amount of rainfall. After plotting points, you find a large grouping of values around 0 cm and 0.2 cm. There is a small grouping of values around 500 cm. What are the reasons for each of these groupings? What should you do to correct these values?",
      "answers": [
        "The groupings around 0 cm are days that had no rainfall, the groupings around 0.2 cm are days where it rained, the groupings around 500 cm are days where it snowed. The values should be used as is.",
        "The groupings around 0 cm are days that had no rainfall, the groupings around 0.2 cm are days where it rained, the groupings around 500 cm are outliers. The values around 500 cm should be dropped and the other values should be used as is.",
        "The groupings around 0 cm are days that had no rainfall, the groupings around 0.2 cm are days where it rained, the groupings around 500 cm are outliers. The values around 500 cm should be normalized so they are on the same scale as the other values.",
        "The groupings around 0 cm and 0.2 cm are extremes and should be removed. The values around 500 cm should be normalized and used once normalized."
      ],
      "correctAnswer": ["The groupings around 0 cm are days that had no rainfall, the groupings around 0.2 cm are days where it rained, the groupings around 500 cm are outliers. The values around 500 cm should be dropped and the other values should be used as is."]
    }
  },
  {
    "id": "336",
    "category": "Cloud Concepts",
    "info": {
      "subcategory": "Exploratory Data Analysis",
      "questionType": "multiple choice 3",
      "question": "Which visualizations help show composition? (Choose 3)",
      "answers": [
        "Pie chart",
        "Stacked area chart",
        "Histogram",
        "Bubble chart",
        "Stacked bar chart",
        "Bar chart",
        "Box plot"
      ],
      "correctAnswer": ["Pie chart",
        "Stacked area chart",
        "Stacked bar chart"]
    }
  },
  {
    "id": "336",
    "category": "Cloud Concepts",
    "info": {
      "subcategory": "Exploratory Data Analysis",
      "questionType": "multiple choice 1",
      "question": "You are working for a major research university analyzing data about the professors who teach there. The features within the data contain information like employee id, position, department, job description, salary, and tenure. The tenure attribute is binary 0 or 1, whether the professor has tenure or does not have tenure. You need to find the distribution of professors and salaries. What is the best visualization to use to achieve this?",
      "answers": [
        "Pie chart",
        "Scatter chart",
        "Histogram",
        "Bubble chart",
        "Line chart"
      ],
      "correctAnswer": ["Histogram"]
    }
  },
  {
    "id": "337",
    "category": "Cloud Concepts",
    "info": {
      "subcategory": "Exploratory Data Analysis",
      "questionType": "multiple choice 1",
      "question": "Which type of visualization uses color to show the density of values?",
      "answers": [
        "Scatter chart",
        "Heatmap",
        "Bubble plot",
        "Histogram"
      ],
      "correctAnswer": ["Heatmap"]
    }
  },
  {
    "id": "338",
    "category": "Cloud Concepts",
    "info": {
      "subcategory": "Exploratory Data Analysis",
      "questionType": "multiple choice 3",
      "question": "Which visualizations help show distribution? (Choose 3)",
      "answers": [
        "Stacked bar chart",
        "Box plot",
        "Bubble chart",
        "Scatter chart",
        "Stacked area chart",
        "Histogram",
        "Line chart"
      ],
      "correctAnswer": ["Box plot",
        "Scatter chart",
        "Histogram"]
    }
  },
  {
    "id": "339",
    "category": "Cloud Concepts",
    "info": {
      "subcategory": "Exploratory Data Analysis",
      "questionType": "multiple choice 1",
      "question": "Which Amazon service allows you to create interactive graphs and charts, and acts as Business Intelligence (BI) tool?",
      "answers": [
        "Athena",
        "Matplotlib",
        "Tableau",
        "Quicksight"
      ],
      "correctAnswer": ["Quicksight"]
    }
  },
  {
    "id": "340",
    "category": "Cloud Concepts",
    "info": {
      "subcategory": "Exploratory Data Analysis",
      "questionType": "multiple choice 2",
      "question": "Which visualizations help show relationships? (Choose 2)",
      "answers": [
        "Bubble chart",
        "Bar chart",
        "Pie chart",
        "Scatter plot",
        "Histogram",
        "Stacked area chart",
        "Stacked bar chart"
      ],
      "correctAnswer": ["Bubble chart",
        "Scatter plot"]
    }
  },
  {
    "id": "341",
    "category": "Cloud Concepts",
    "info": {
      "subcategory": "Exploratory Data Analysis",
      "questionType": "multiple choice 2",
      "question": "Which visualizations help show comparisons? (Choose 2)",
      "answers": [
        "Bar chart",
        "Stacked area chart",
        "Line chart",
        "Stacked bar chart",
        "Scatter plot",
        "Histogram",
        "Bubble chart"
      ],
      "correctAnswer": ["Bar chart",
        "Line chart"]
    }
  },
  {
    "id": "342",
    "category": "Cloud Concepts",
    "info": {
      "subcategory": "Exploratory Data Analysis",
      "questionType": "multiple choice 1",
      "question": "You are an ML specialist working for a retail organization. You are analyzing customer spending data for particular locations and comparing how it changes over time. You want to visualize the monthly total amount spent at each location over the last 5 years. Which visualization can you use to help you see this?",
      "answers": [
        "Histogram",
        "Bar chart",
        "Line chart",
        "Scatter chart"
      ],
      "correctAnswer": ["Line chart"]
    }
  },
  {
    "id": "343",
    "category": "Cloud Concepts",
    "info": {
      "subcategory": "Exploratory Data Analysis",
      "questionType": "multiple choice 1",
      "question": "You are an ML specialist designing a regression model to predict the sales for an upcoming festival. The data from the past consists of 1,000 records containing 20 numeric attributes. As you start to analyze the data, you discovered that 30 records have values that are in the far left of a box plot's lower quartile. The festival manager confirmed that those values are unusual, but plausible. There are also 65 records where another numerical value is blank. What should you do to correct these problems?",
      "answers": [
        "Drop the unusual records and replace the blank values with the mean value.",
        "Drop the unusual records and replace the blank values with separate Boolean values.",
        "Use the unusual data and replace the missing values with a separate Boolean variable.",
        "Drop the unusual records and fill in the blank values with 0."
      ],
      "correctAnswer": ["Drop the unusual records and replace the blank values with the mean value."]
    }
  },
  {
    "id": "344",
    "category": "Cloud Concepts",
    "info": {
      "subcategory": "Exploratory Data Analysis",
      "questionType": "multiple choice 1",
      "question": "You are an ML specialist working for a retail organization. You are analyzing data that has different items at different costs. You decide to choose the top 5 most expensive items and visually compare their prices. Which visualization can help you achieve this?",
      "answers": [
        "Bar chart",
        "Pie chart",
        "Scatter chart",
        "Histogram"
      ],
      "correctAnswer": ["Bar chart"]
    }
  },
  {
    "id": "345",
    "category": "Cloud Concepts",
    "info": {
      "subcategory": "Exploratory Data Analysis",
      "questionType": "multiple choice 1",
      "question": "What does the box in a box plot represent?",
      "answers": [
        "The middle 50% of the values.",
        "The minimum values.",
        "The maximum values.",
        "The median value."
      ],
      "correctAnswer": ["The middle 50% of the values."]
    }
  },
  {
    "id": "346",
    "category": "Cloud Concepts",
    "info": {
      "subcategory": "Modeling",
      "questionType": "multiple choice 1",
      "question": "We are using a CSV dataset for unsupervised learning that does not include a target value. How should we indicate this for training data as it sits on S3?",
      "answers": [
        "SageMaker will automatically detect the data format for supervised learning algorithms.",
        "CSV data format should not be used for unsupervised learning algorithms.",
        "Enable pipe mode when we initiate the training run.",
        "Include a reserved word metadata key of ColumnCount for the S3 file and set it to the number of columns.",
        "Include label_size=0 appended to the Content-Type key."
      ],
      "correctAnswer": ["Include label_size=0 appended to the Content-Type key."]
    }
  },
  {
    "id": "347",
    "category": "Cloud Concepts",
    "info": {
      "subcategory": "Modeling",
      "questionType": "multiple choice 1",
      "question": "You have launched a training job but it fails after a few minutes. What is the first thing you should do for troubleshooting?",
      "answers": [
        "Go to CloudWatch logs and try to identify the error in the logs for your job.",
        "Go to CloudTrail logs and try to identify the error in the logs for your job.",
        "Ensure that your instance type is large enough and resubmit the job in a different region.",
        "Check to see that your Notebook instance has the proper permissions to access the input files on S3.",
        "Submit the job with AWS X-Ray enabled for additional debug information."
      ],
      "correctAnswer": ["Go to CloudWatch logs and try to identify the error in the logs for your job."]
    }
  },
  {
    "id": "348",
    "category": "Cloud Concepts",
    "info": {
      "subcategory": "Modeling",
      "questionType": "multiple choice 1",
      "question": "We are designing a binary classification model that tries to predict whether a customer is likely to respond to a direct mailing of our catalog. Because it is expensive to print and mail our catalog, we want to only send to customers where we have a high degree of certainty they will buy something. When considering if the customer will buy something, what outcome would we want to minimize in a confusion matrix?",
      "answers": [
        "False Negative",
        "False Positive",
        "True Negative",
        "True Positive",
        "False Affirmative"
      ],
      "correctAnswer": ["False Positive"]
    }
  },
  {
    "id": "349",
    "category": "Cloud Concepts",
    "info": {
      "subcategory": "Modeling",
      "questionType": "multiple choice 1",
      "question": "You want to be sure to use the most stable version of a training container. How do you ensure this?",
      "answers": [
        "Use the path to the global container repository.",
        "Use the :latest tag when specifying the ECR container path.",
        "Use the :1 tag when specifying the ECR container path.",
        "Use the ECR repository located in US-EAST-2."
      ],
      "correctAnswer": ["Use the :1 tag when specifying the ECR container path."]
    }
  },
  {
    "id": "350",
    "category": "Cloud Concepts",
    "info": {
      "subcategory": "Modeling",
      "questionType": "multiple choice 1",
      "question": "When you issue a CreateModel API call using a built-in algorithm, which of the following actions would be next?",
      "answers": [
        "SageMaker launches an appropriate inference container for the algorithm selected from the regional container repository.",
        "SageMaker launches an appropriate training container from the algorithm selected from the regional container repository.",
        "Sagemaker provisions an EC2 instances using the appropriate AMI for the algorithm selected from the regional container registry.",
        "SageMaker provisions an EMR cluster and prepares a Spark script for the training job.",
        "Sagemaker provisions an EC2 instances using the appropriate AMI for the algorithm selected from the global container registry.",
        "SageMaker launches an appropriate inference container for the algorithm selected from the global container repository."
      ],
      "correctAnswer": ["SageMaker launches an appropriate inference container for the algorithm selected from the regional container repository."]
    }
  },
  {
    "id": "351",
    "category": "Cloud Concepts",
    "info": {
      "subcategory": "Modeling",
      "questionType": "multiple choice 1",
      "question": "We are using a k-fold method of cross-validation for our linear regression model. What outcome will indicate that our training data is not biased?",
      "answers": [
        "Each subsequent k-fold validation round has an increasing accuracy rate over the one prior.",
        "Bias is not a concern with linear regression problems as the error function resolves this.",
        "K-fold is not appropriate for us with linear regression problems.",
        "All k-fold validation rounds have roughly the same error rate.",
        "Each subsequent k-fold validation round has a decreasing error rate over the one prior."
      ],
      "correctAnswer": ["All k-fold validation rounds have roughly the same error rate."]
    }
  },
  {
    "id": "352",
    "category": "Cloud Concepts",
    "info": {
      "subcategory": "Modeling",
      "questionType": "multiple choice 2",
      "question": "We are running a training job over and over again using slightly different, very large datasets as an experiment. Training is taking a very long time with your I/O-bound training algorithm and you want to improve training performance. What might you consider? (Choose 2)",
      "answers": [
        "Make use of pipe mode to stream data directly from S3.",
        "Convert the data format to an Integer32 tensor.",
        "Use the SageMaker console to change your training job instance type from an ml.c5.xlarge to a r5.xlarge.",
        "Convert the data format to protobuf recordIO format.",
        "Make use of file mode to stream data directly from S3."
      ],
      "correctAnswer": ["Make use of pipe mode to stream data directly from S3.",
        "Convert the data format to protobuf recordIO format."]
    }
  },
  {
    "id": "353",
    "category": "Cloud Concepts",
    "info": {
      "subcategory": "Modeling",
      "questionType": "multiple choice 1",
      "question": "You have been provided with a cleansed and prepared dataset you will be using for a linear regression model. Of these tasks, which would you do first?",
      "answers": [
        "Run a randomization process on the data.",
        "Split the data into testing and training datasets.",
        "Run a Peterman distribution on the data to sort it properly for linear regression.",
        "Perform one-hot encoding on the softmax results."
      ],
      "correctAnswer": ["Run a randomization process on the data."]
    }
  },
  {
    "id": "354",
    "category": "Cloud Concepts",
    "info": {
      "subcategory": "Modeling",
      "questionType": "multiple choice 1",
      "question": "You are working on a model that tries to predict the future revenue of select companies based on 50 years of historic data from public financial filings. What might be a strategy to determine if the model is reasonably accurate?",
      "answers": [
        "Use Random Cut Forest to remove any outliers and rerun the algorithm on the last 20% of the data.",
        "Randomize the training data and reserve 20% as a validation set after the training process is completed.",
        "Use a set of the historic data as testing data to back-test the model and compare results to actual historical results.",
        "Use a softmax function to invert the historical data then run the validation job from most recent to earliest history."
      ],
      "correctAnswer": ["Use a set of the historic data as testing data to back-test the model and compare results to actual historical results."]
    }
  },
  {
    "id": "355",
    "category": "Cloud Concepts",
    "info": {
      "subcategory": "Modeling",
      "questionType": "multiple choice 1",
      "question": "Your company currently has a large on-prem Hadoop cluster that contains data you would like to use for a training job. Your cluster is equipped with Mahout, Flume, Hive, Spark, and Ganglia. How might you most efficiently use this data?",
      "answers": [
        "Using EMR, create a Scala script to export the data to an HDFS volume. Copy that data over to an EBS volume where it can be read by the SageMaker training containers.",
        "Use Mahout on the Hadoop Cluster to preprocess the data into a format that is compatible with SageMaker. Export the data with Flume to the local storage of the training container and launch the training job.",
        "Ensure that Spark is supported on your Hadoop cluster and leverage the SageMaker Spark library.",
        "Use Data Pipeline to make a copy of the data in Spark DataFrame format. Upload the data to S3 where it can be accessed by the SageMaker training jobs."
      ],
      "correctAnswer": ["Ensure that Spark is supported on your Hadoop cluster and leverage the SageMaker Spark library."]
    }
  },
  {
    "id": "356",
    "category": "Cloud Concepts",
    "info": {
      "subcategory": "Modeling",
      "questionType": "multiple choice 1",
      "question": "You are consulting for a mountain climbing gear manufacturer and have been asked to design a machine learning approach for predicting the strength of a new line of climbing ropes. Which approach might you choose?",
      "answers": [
        "You would choose a multi-class classification approach to classify the rope into an appropriate price range.",
        "You would choose a simulation-based reinforcement learning approach.",
        "You would choose a binary classification approach to determine if the rope will fail or not.",
        "You would recommend they do not use a machine learning model.",
        "You would approach the problem as a linear regression problem to predict the tensile strength of the rope based on other ropes."
      ],
      "correctAnswer": ["You would recommend they do not use a machine learning model."]
    }
  },
  {
    "id": "357",
    "category": "Cloud Concepts",
    "info": {
      "subcategory": "Modeling",
      "questionType": "multiple choice 1",
      "question": "Which of the following mean that our algorithm predicted false but the real outcome was true?",
      "answers": [
        "False Negative",
        "False Positive",
        "True Positive",
        "False Affirmative",
        "True Negative"
      ],
      "correctAnswer": ["False Negative"]
    }
  }
]