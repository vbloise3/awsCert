[
  {
    "id": "293",
    "category": "Cloud Concepts",
    "info": {
      "subcategory": "Data Collection",
      "questionType": "multiple choice 1",
      "question": "You have been tasked with setting up crawlers in AWS Glue to crawl different data stores to populate your organizations AWS Glue Data Catalogs. Which of the following input data store is NOT an option when creating a crawler?",
      "answers": [
        "JDBC Connections",
        "DocumentDB",
        "DynamoDB",
        "S3",
        "RDS",
        "RedShift"
      ],
      "correctAnswer": ["DocumentDB"]
    }
  },
  {
    "id": "294",
    "category": "Cloud Concepts",
    "info": {
      "subcategory": "Data Collection",
      "questionType": "multiple choice 1",
      "question": "You are a ML specialist working with data that is stored in a distributed EMR cluster on AWS. Currently, your machine learning applications are compatible with the Apache Hive Metastore tables on EMR. You have been tasked with configuring Hive to use the AWS Glue Data Catalog as its metastore. Before you can do this you need to transfer the Apache Hive metastore tables into an AWS Glue Data Catalog. What are the steps you will need to take to achieve this with the LEAST amount of effort?",
      "answers": [
        "Create DMS endpoints for both the input Apache Hive Metastore and the output data store S3 bucket, run a DMS migration to transfer the data, then create a crawler that creates an AWS Glue Data Catalog.",
        "Set up your Apache Hive application with JDBC driver connections, then create a crawler that crawlers the Apache Hive Metastore using the JDBC connection and creates an AWS Glue Data Catalog.",
        "Run a Hive script on EMR that reads from your Apache Hive Metastore, exports the data to an intermediate format in Amazon S3, and then imports that data into the AWS Glue Data Catalog.",
        "Create a Data Pipeline job that reads from your Apache Hive Metastore, exports the data to an intermediate format in Amazon S3, and then imports that data into the AWS Glue Data Catalog."
      ],
      "correctAnswer": ["Run a Hive script on EMR that reads from your Apache Hive Metastore, exports the data to an intermediate format in Amazon S3, and then imports that data into the AWS Glue Data Catalog."]
    }
  },
  {
    "id": "295",
    "category": "Cloud Concepts",
    "info": {
      "subcategory": "Data Collection",
      "questionType": "multiple choice 1",
      "question": "Your organization has given you several different sets of key-value pair JSON files that need to be used for a machine learning project within AWS. What type of data is this classified as and where is the best place to load this data into?",
      "answers": [
        "Structured data, stored in RDS.",
        "Unstructured data, stored in S3.",
        "Semi-structured data, stored in DynamoDB.",
        "Semi-structured data, stored in S3."
      ],
      "correctAnswer": ["Semi-structured data, stored in S3."]
    }
  },
  {
    "id": "296",
    "category": "Cloud Concepts",
    "info": {
      "subcategory": "Data Collection",
      "questionType": "multiple choice 1",
      "question": "You have been tasked with converting multiple JSON files within an S3 bucket to Apache Parquet format. Which AWS service can you use to achieve this with the LEAST amount of effort?",
      "answers": [
        "Create an AWS Glue Job to convert the S3 objects from JSON to Apache Parquet, then output newly formatted files into S3.",
        "Create a Lambda function that reads all of the objects in the S3 bucket. Loop through each of the objects and convert from JSON to Apache Parquet. Once the conversion is complete, output newly formatted files into S3.",
        "Create a Data Pipeline job that reads from your S3 bucket and sends the data the EMR. Create an Apache Spark job to process the data the Apache Parquet and output newly formatted files into S3.",
        "Create an EMR cluster to run an Apache Spark job to process the data the Apache Parquet and output newly formatted files into S3."
      ],
      "correctAnswer": ["Create an AWS Glue Job to convert the S3 objects from JSON to Apache Parquet, then output newly formatted files into S3."]
    }
  },
  {
    "id": "297",
    "category": "Cloud Concepts",
    "info": {
      "subcategory": "Data Collection",
      "questionType": "multiple choice 1",
      "question": "You are a ML specialist within a large organization who needs to run SQL queries and analytics on thousands of Apache logs files stored in S3. Which set of tools can help you achieve this with the LEAST amount of effort?",
      "answers": [
        "AWS Glue Data Catalog and Athena",
        "Redshift and Redshift Spectrum",
        "Data Pipeline and Athena",
        "Data Pipeline and RDS"
      ],
      "correctAnswer": ["AWS Glue Data Catalog and Athena"]
    }
  },
  {
    "id": "298",
    "category": "Cloud Concepts",
    "info": {
      "subcategory": "Data Collection",
      "questionType": "multiple choice 1",
      "question": "In general within your dataset, what is the minimum number of observations you should have compared to the number of features?",
      "answers": [
        "10,000 times as many observations as features.",
        "10 times as many observations as features.",
        "100 times as many observations as features.",
        "1000 times as many observations as features."
      ],
      "correctAnswer": ["10 times as many observations as features."]
    }
  },
  {
    "id": "299",
    "category": "Cloud Concepts",
    "info": {
      "subcategory": "Data Collection",
      "questionType": "multiple choice 1",
      "question": "Which Amazon service allows you to build a high-quality training labeled dataset for your machine learning models? This includes human workers, vendor companies that you choose, or an internal, private workforce.",
      "answers": [
        "S3",
        "Lambda",
        "SageMaker Ground Truth",
        "Jupyter Notebooks"
      ],
      "correctAnswer": ["SageMaker Ground Truth"]
    }
  },
  {
    "id": "300",
    "category": "Cloud Concepts",
    "info": {
      "subcategory": "Data Collection",
      "questionType": "multiple choice 1",
      "question": "An organization needs to store a mass amount of data in AWS. The data has a key-value access pattern, developers need to run complex SQL queries and transactions, and the data has a fixed schema. Which type of data store meets all of their needs?",
      "answers": [
        "Athena",
        "DynamoDB",
        "RDS",
        "S3"
      ],
      "correctAnswer": ["RDS"]
    }
  },
  {
    "id": "301",
    "category": "Cloud Concepts",
    "info": {
      "subcategory": "Data Collection",
      "questionType": "multiple choice 1",
      "question": "You are trying to set up a crawler within AWS Glue that crawls your input data in S3. For some reason after the crawler finishes executing, it cannot determine the schema from your data and no tables are created within your AWS Glue Data Catalog. What is the reason for these results?",
      "answers": [
        "AWS Glue built-in classifiers could not find the input data format. You need to create a custom classifier.",
        "The bucket path for the input data store in S3 is specified incorrectly.",
        "The checkbox for Do not create tables was checked when setting up the crawler in AWS Glue.",
        "The crawler does not have correct IAM permissions to access the input data in the S3 bucket."
      ],
      "correctAnswer": ["AWS Glue built-in classifiers could not find the input data format. You need to create a custom classifier."]
    }
  },
  {
    "id": "302",
    "category": "Cloud Concepts",
    "info": {
      "subcategory": "Data Collection",
      "questionType": "multiple choice 1",
      "question": "When you train your model in SageMaker, where does your training dataset come from?",
      "answers": [
        "S3",
        "RedShift",
        "DynamoDb",
        "RDS"
      ],
      "correctAnswer": ["S3"]
    }
  },
  {
    "id": "303",
    "category": "Cloud Concepts",
    "info": {
      "subcategory": "Data Collection",
      "questionType": "multiple choice 1",
      "question": "You are a ML specialist within a large organization who needs to run SQL queries and analytics on thousands of Apache logs files stored in S3. Your organization already uses Redshift as their data warehousing solution. Which tool can help you achieve this with the LEAST amount of effort?",
      "answers": [
        "S3 Analytics",
        "Redshift Spectrum",
        "Athena",
        "Apache Hive"
      ],
      "correctAnswer": ["Redshift Spectrum"]
    }
  },
  {
    "id": "304",
    "category": "Cloud Concepts",
    "info": {
      "subcategory": "Data Collection",
      "questionType": "multiple choice 1",
      "question": "You are a ML specialist within a large organization who helps job seekers find both technical and non-technical jobs. You have collected data from a data warehouse from an engineering company to determine which skills qualify job seekers for different positions. After reviewing the data you realise the data is biased. Why?",
      "answers": [
        "The data collected needs to be from the general population of job seekers, not just from a technical engineering company.",
        "The data collected is only a few hundred observations making it bias to a small subset of job types.",
        "The data collected has missing values for different skills for job seekers.",
        "The data collected only has a few attributes. Attributes like skills and job title are not included in the data."
      ],
      "correctAnswer": ["The data collected needs to be from the general population of job seekers, not just from a technical engineering company."]
    }
  },
  {
    "id": "305",
    "category": "Cloud Concepts",
    "info": {
      "subcategory": "Data Collection",
      "questionType": "multiple choice 1",
      "question": "You have been tasked with collecting thousands of PDFs for building a large corpus dataset. The data within this dataset would be considered what type of data?",
      "answers": [
        "Structured",
        "Relational",
        "Unstructured",
        "Semi-structured"
      ],
      "correctAnswer": ["Unstructured"]
    }
  },
  {
    "id": "306",
    "category": "Cloud Concepts",
    "info": {
      "subcategory": "Data Collection",
      "questionType": "multiple choice 1",
      "question": "You are a ML specialist who is setting up an ML pipeline. The amount of data you have is massive and needs to be set up and managed on a distributed system to efficiently run processing and analytics on. You also plan to use tools like Apache Spark to process your data to get it ready for your ML pipeline. Which setup and services can most easily help you achieve this?",
      "answers": [
        "Redshift out-performs Apache Spark and should be used instead.",
        "Elastic Map Reduce (EMR) with Apache Spark installed.",
        "Self-managed cluster of EC2 instances with Apache Spark installed.",
        "Multi AZ RDS Read Replicas with Apache Spark installed."
      ],
      "correctAnswer": ["Elastic Map Reduce (EMR) with Apache Spark installed."]
    }
  },
  {
    "id": "306",
    "category": "Cloud Concepts",
    "info": {
      "subcategory": "Streaming Data",
      "questionType": "multiple choice 1",
      "question": "Which service in the Kinesis family allows you to securely stream video from connected devices to AWS for analytics, machine learning (ML), and other processing?",
      "answers": [
        "Kinesis Video Streams",
        "Kinesis Streams",
        "Kinesis Firehose",
        "Kinesis Data Analytics"
      ],
      "correctAnswer": ["Kinesis Video Streams"]
    }
  },
  {
    "id": "306",
    "category": "Cloud Concepts",
    "info": {
      "subcategory": "Streaming Data",
      "questionType": "multiple choice 1",
      "question": "Which service built by AWS makes it easy to set up a retry mechanism, aggregate records to improve throughput, and automatically submits CloudWatch metrics?",
      "answers": [
        "Kinesis Client Library (KCL)",
        "Kinesis Producer Library (KPL)",
        "Kinesis Consumer Library",
        "Kinesis API (AWS SDK)"
      ],
      "correctAnswer": ["Kinesis Producer Library (KPL)"]
    }
  },
  {
    "id": "307",
    "category": "Cloud Concepts",
    "info": {
      "subcategory": "Streaming Data",
      "questionType": "multiple choice 1",
      "question": "Your organization has a standalone Javascript (Node.js) application that streams data into AWS using Kinesis Data Streams. You notice that they are using the Kinesis API (AWS SDK) over the Kinesis Producer Library (KPL). What might be the reasoning behind this?",
      "answers": [
        "The Kinesis Producer Library cannot be integrated with a Javascript application because of its asynchronous architecture.",
        "The Kinesis API (AWS SDK) runs faster in Javascript applications over the Kinesis Producer Library.",
        "The Kinesis Producer Library must be installed as a Java application to use with Kinesis Data Streams.",
        "The Kinesis API (AWS SDK) provides greater functionality over the Kinesis Producer Library."
      ],
      "correctAnswer": ["The Kinesis Producer Library must be installed as a Java application to use with Kinesis Data Streams."]
    }
  },
  {
    "id": "308",
    "category": "Cloud Concepts",
    "info": {
      "subcategory": "Streaming Data",
      "questionType": "multiple choice 1",
      "question": "You are collecting clickstream data from an e-commerce website using Kinesis Data Firehose. You are using the PutRecord API from the AWS SDK to send the data to the stream. What are the required parameters when sending data to Kinesis Data Firehose using the API PutRecord call?",
      "answers": [
        "DataStreamName, PartitionKey, and Record (containing the data)",
        "Data, PartitionKey, StreamName, ShardId",
        "Data, PartitionKey, StreamName",
        "DeliveryStreamName and Record (containing the data)"
      ],
      "correctAnswer": ["DeliveryStreamName and Record (containing the data)"]
    }
  },
  {
    "id": "309",
    "category": "Cloud Concepts",
    "info": {
      "subcategory": "Streaming Data",
      "questionType": "multiple choice 3",
      "question": "You work for a farming company that has dozens of tractors with build-in IoT devices. These devices stream data into AWS using Kinesis Data Streams. The features associated with the data is tractor Id, latitude, longitude, inside temp, outside temp, and fuel level. As an ML specialist you need to transform the data and store it in a data store. Which combination of services can you use to achieve this? (Choose 3)",
      "answers": [
        "Use Kinesis Data Streams to immediately write the data into S3. Next, set up a Lambda function that fires any time an object is PUT onto S3. Transform the data from the Lambda function, then write the transformed data into S3.",
        "Immediately send the data to Lambda from Kinesis Data Streams. Transform the data in Lambda and write the transformed data into S3.",
        "Use Kinesis Data Analytics to run real-time SQL queries to transform the data and immediately write the transformed data into S3.",
        "Set up Kinesis Firehose to ingest data from Kinesis Data Streams, then send data to Lambda. Transform the data in Lambda and write the transformed data into S3.",
        "Set up Kinesis Data Analytics to ingest the data from Kinesis Data Stream, then run real-time SQL queries on the data to transform it. After the data is transformed, ingest the data with Kinesis Data Firehose and write the data into S3."
      ],
      "correctAnswer": ["Immediately send the data to Lambda from Kinesis Data Streams. Transform the data in Lambda and write the transformed data into S3.",
        "Set up Kinesis Firehose to ingest data from Kinesis Data Streams, then send data to Lambda. Transform the data in Lambda and write the transformed data into S3.",
        "Set up Kinesis Data Analytics to ingest the data from Kinesis Data Stream, then run real-time SQL queries on the data to transform it. After the data is transformed, ingest the data with Kinesis Data Firehose and write the data into S3."]
    }
  },
  {
    "id": "310",
    "category": "Cloud Concepts",
    "info": {
      "subcategory": "Streaming Data",
      "questionType": "multiple choice 1",
      "question": "Your organization needs to find a way to capture streaming data from certain events customers are performing. These events are a crucial part of the organizations business development and cannot afford to be lost. You have already set up a Kinesis Data Stream and a consumer EC2 instance to process and deliver the data into S3. You have noticed that the last few days of events are not showing up in S3 and your EC2 instance has been shutdown. What combination of steps can you take to ensure this does not happen again?",
      "answers": [
        "Set up CloudWatch monitoring for your EC2 instance as well as AutoScaling if your consumer EC2 instance is shutdown. Next, set up multiple Kinesis Data Streams to process the data on the EC2 instance.",
        "Set up CloudWatch monitoring for your EC2 instance as well as AutoScaling if your consumer EC2 instance is shutdown. Next, set up a Lambda function to poll the Kinesis Data Stream for failed delivered records and then send those requests back into the consumer EC2 instance.",
        "Set up CloudWatch monitoring for your EC2 instance as well as AutoScaling if your consumer EC2 instance is shutdown. Next, send the data to Kinesis Data Firehose before writing the data into S3. Since Kinesis Data Firehose has retry mechanism built-in, the changes of data being lost is extremely unlikely.",
        "Set up CloudWatch monitoring for your EC2 instance as well as AutoScaling if your consumer EC2 instance is shutdown. Next, ensure that the maximum amount of hours are selected (168 hours) for data retention when creating your Kinesis Data Stream. Finally, write logic on the consumer EC2 instance that handles unprocessed data in the Kinesis Data Stream and failed writes to S3."
      ],
      "correctAnswer": ["Set up CloudWatch monitoring for your EC2 instance as well as AutoScaling if your consumer EC2 instance is shutdown. Next, ensure that the maximum amount of hours are selected (168 hours) for data retention when creating your Kinesis Data Stream. Finally, write logic on the consumer EC2 instance that handles unprocessed data in the Kinesis Data Stream and failed writes to S3."]
    }
  },
  {
    "id": "311",
    "category": "Cloud Concepts",
    "info": {
      "subcategory": "Streaming Data",
      "questionType": "multiple choice 1",
      "question": "Which service in the Kinesis family allows you to easily load streaming data into data stores and analytics tools?",
      "answers": [
        "Kinesis Streams",
        "Kinesis Firehose",
        "Kinesis Video Streams",
        "Kinesis Data Analytics"
      ],
      "correctAnswer": ["Kinesis Firehose"]
    }
  },
  {
    "id": "312",
    "category": "Cloud Concepts",
    "info": {
      "subcategory": "Streaming Data",
      "questionType": "multiple choice 1",
      "question": "You have been tasked with capturing data from an online gaming platform to run analytics on and process through a machine learning pipeline. The data that you are ingesting is players controller inputs every 1 second (up to 10 players in a game) that is in JSON format. The data needs to be ingested through Kinesis Data Streams and the JSON data blob is 100 KB in size. What is the minimum number of shards you can use to successfully ingest this data?",
      "answers": [
        "Greater than 500 shards, so you'll need to request more shards from AWS",
        "10 shards",
        "1 shard",
        "100 shards"
      ],
      "correctAnswer": ["1 shard"]
    }
  },
  {
    "id": "313",
    "category": "Cloud Concepts",
    "info": {
      "subcategory": "Streaming Data",
      "questionType": "multiple choice 1",
      "question": "True or False. If you have mission critical data that must be processed with as minimal delay as possible, you should use the Kinesis API (AWS SDK) over the Kinesis Producer Library.",
      "answers": [
        "true",
        "false"
      ],
      "correctAnswer": ["true"]
    }
  },
  {
    "id": "314",
    "category": "Cloud Concepts",
    "info": {
      "subcategory": "Streaming Data",
      "questionType": "multiple choice 1",
      "question": "Which service in the Kinesis family allows you to build custom applications that process or analyze streaming data for specialized needs?",
      "answers": [
        "Kinesis Video Streams",
        "Kinesis Firehose",
        "Kinesis Data Analytics",
        "Kinesis Streams"
      ],
      "correctAnswer": ["Kinesis Streams"]
    }
  },
  {
    "id": "315",
    "category": "Cloud Concepts",
    "info": {
      "subcategory": "Streaming Data",
      "questionType": "multiple choice 1",
      "question": "You are collecting clickstream data from an e-commerce website to make near-real time product suggestions for users actively using the site. Which combination of tools can be used to achieve the quickest recommendations and meets all of the requirements?",
      "answers": [
        "Use the Kinesis Data Analytics to ingest the clickstream data directly and run real time SQL queries to gain actionable insights and trigger real-time recommendations with AWS Lambda functions based on conditions.",
        "Use Kinesis Data Firehose to ingest click stream data, then use Kinesis Data Analytics to run real time SQL queries to gain actionable insights and trigger real-time recommendations with AWS Lambda functions based on conditions, then use Lambda to load these results into S3.",
        "Use Kinesis Data Streams to ingest clickstream data, then use Lambda to process that data and write it to S3. Once the data is on S3, use Athena to query based on conditions that data and make real time recommendations to users.",
        "Use Kinesis Data Streams to ingest clickstream data, then use Kinesis Data Analytics to run real time SQL queries to gain actionable insights and trigger real-time recommendations with AWS Lambda functions based on conditions."
      ],
      "correctAnswer": ["Use Kinesis Data Streams to ingest clickstream data, then use Kinesis Data Analytics to run real time SQL queries to gain actionable insights and trigger real-time recommendations with AWS Lambda functions based on conditions."]
    }
  },
  {
    "id": "316",
    "category": "Cloud Concepts",
    "info": {
      "subcategory": "Streaming Data",
      "questionType": "multiple choice 1",
      "question": "You are an ML specialist needing to collect data from Twitter tweets. You only want to collect tweets that include the name of your company and store those tweets off into a data store in AWS. What set of tools can you use to stream, transform, and load the data into AWS with the LEAST amount of effort?",
      "answers": [
        "Setup a Kinesis Data Stream for data ingestion, setup EC2 instances as data consumers to poll and transform the data from the stream. Once the data is transformed, make an API call to write the data to DynamoDB.",
        "Setup Kinesis Data Streams for data ingestion. Next, setup Kinesis Data Firehouse to load that data into RedShift. Next, setup a Lambda function to query data using RedShift spectrum and store the results onto DynamoDB.",
        "Create a Kinesis Data Stream to ingest the data. Next, setup a Kinesis Data Firehose and use Lambda to transform the data from the Kinesis Data Stream, then use Lambda to write the data to DynamoDB. Finally, use S3 as the data destination for Kinesis Data Firehose.",
        "Setup a Kinesis Data Firehose for data ingestion and immediately write that data to S3. Next, setup a Lambda function to trigger when data lands in S3 to transform it and finally write it to DynamoDB."
      ],
      "correctAnswer": ["Setup a Kinesis Data Firehose for data ingestion and immediately write that data to S3. Next, setup a Lambda function to trigger when data lands in S3 to transform it and finally write it to DynamoDB."]
    }
  },
  {
    "id": "317",
    "category": "Cloud Concepts",
    "info": {
      "subcategory": "Streaming Data",
      "questionType": "multiple choice 1",
      "question": "Which service in the Kinesis family allows you to analyze streaming data, gain actionable insights, and respond to your business and customer needs in real time?",
      "answers": [
        "Kinesis Video Streams",
        "Kinesis Data Analytics",
        "Kinesis Streams",
        "Kinesis Firehose"
      ],
      "correctAnswer": ["Kinesis Data Analytics"]
    }
  },
  {
    "id": "318",
    "category": "Cloud Concepts",
    "info": {
      "subcategory": "Streaming Data",
      "questionType": "multiple choice 1",
      "question": "A local university wants to track cars in a parking lot to determine which students are parking in the lot. The university is wanting to ingest videos of the cars parking in near-real time, use machine learning to identify license plates, and store that data in an AWS data store. Which solution meets these requirements with the LEAST amount of development effort?",
      "answers": [
        "Use Amazon Kinesis Data Streams to ingest videos in near-real time, call Amazon Rekognition to identify license plate information, and then store results in DynamoDB.",
        "Use Amazon Kinesis Data Streams to ingest the video in near-real time, use the Kinesis Data Streams consumer integrated with Amazon Rekognition Video to process the license plate information, and then store results in DynamoDB.",
        "Use Amazon Kinesis Firehose to ingest the video in near-real time and outputs results onto S3. Set up a Lambda function that triggers when a new video is PUT onto S3 to send results to Amazon Rekognition to identify license plate information, and then store results in DynamoDB.",
        "Use Amazon Kinesis Video Streams to ingest the videos in near-real time, use the Kinesis Video Streams integration with Amazon Rekognition Video to identify the license plate information, and then store the results in DynamoDB."
      ],
      "correctAnswer": ["Use Amazon Kinesis Video Streams to ingest the videos in near-real time, use the Kinesis Video Streams integration with Amazon Rekognition Video to identify the license plate information, and then store the results in DynamoDB."]
    }
  },
  {
    "id": "319",
    "category": "Cloud Concepts",
    "info": {
      "subcategory": "Streaming Data",
      "questionType": "multiple choice 1",
      "question": "You have been tasked with capturing two different types of streaming events. The first event type includes mission critical data that needs to immediately be processed before operations can continue. The second event type includes data of less importance, but processing can continue without processing. What is the most appropriate solution to record these different types of events?",
      "answers": [
        "Capture both event types using the Kinesis Producer Library (KPL).",
        "Capture the mission critical events with the PutRecords API call and the second event type with the Kinesis Producer Library (KPL).",
        "Capture both events with the PutRecords API call.",
        "Capture the mission critical events with the Kinesis Producer Library (KPL) and the second event type with the Putrecords API call."
      ],
      "correctAnswer": ["Capture the mission critical events with the PutRecords API call and the second event type with the Kinesis Producer Library (KPL)."]
    }
  },
  {
    "id": "320",
    "category": "Cloud Concepts",
    "info": {
      "subcategory": "Streaming Data",
      "questionType": "multiple choice 3",
      "question": "What are your options for storing data into S3? (Choose 3)",
      "answers": [
        "UPLOAD command",
        "The AWS console",
        "PutRecords API call",
        "AWS CLI",
        "UNLOAD command",
        "AWS SDK"
      ],
      "correctAnswer": ["The AWS console",
        "AWS CLI",
        "AWS SDK"]
    }
  },
  {
    "id": "321",
    "category": "Cloud Concepts",
    "info": {
      "subcategory": "Data Engineering",
      "questionType": "multiple choice 1",
      "question": "You are na ML specialist preparing some labeled data to help determine whether a given leaf originates from a poisonous plant. The target attribute is poisonous and is classified as 0 or 1. The data that you have been analyzing has the following features: leaf height (cm), leaf length (cm), number of cells (trillions), poisonous (binary). After initial analysis you do not suspect any outliers in any of the attributes. After using the data given to train your model, you are getting extremely skewed results. What technique can you apply to possibly help solve this issue?",
      "answers": [
        "Drop the number of cells attribute.",
        "Apply one-hot encoding to each of the attributes, except for the poisonous attribute (since it is already encoded).",
        "PutRecords API call",
        "Standardize the number of cells attribute.",
        "Normalize the number of cells attribute."
      ],
      "correctAnswer": ["Normalize the number of cells attribute."]
    }
  }
]